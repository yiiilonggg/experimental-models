{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational AutoEncoder Chess Position Generator\n",
    "\n",
    "##### Inspiration\n",
    "* Recently, I have been reading up about generative models, and one of them that caught my eye was the VAE.\n",
    "* It allows you to generate new data that is similar to your training data.\n",
    "* At the same time, I am interested in chess and have enjoyed solving chess puzzles for quite awhile.\n",
    "* However, the premise of a chess puzzle is that the player knows that there exists a optimal move / sequence of moves that provides the player an advantage.\n",
    "* This helps the player to improve in terms of tactics and pattern recognition, but in most cases when playing a game of chess, we do not know if there exists an optimal solution.\n",
    "* This introduces the idea of an anti-puzzle, where the premise is now that the chess position provided may have an optimal solution, or the \"solution\" is to play a move that maintains the status-quo.\n",
    "* With the VAE, we can train it with a training set of legal chess positions, and have it output more chess positions.\n",
    "* Since the VAE would not have any idea if the chess position has an optimal solution or not, it is perfect for creating \"anti-puzzle\" solutions.\n",
    "* Furthermore, chess is a \"constrained\" game, where the rules are clear and we can check if the position generated by the VAE is a legal position or not.\n",
    "* For this model, the goal is to simply generate new (legal) chess positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 01:49:58.550242: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from scipy.stats import norm\n",
    "from keras import layers, models, metrics, losses, optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Collection\n",
    "\n",
    "* The easiest way to obtain chess is positions is from my own games.\n",
    "* I exported move data from some chess games that I have played online in Lichess, which comes in a .pgn file.\n",
    "* From this file, we can get the move orders for the games that I have exported, from which I can deduce the chess positions.\n",
    "* For this, I used the python-chess library, which helps to deduce FEN positions from PGN move list\n",
    "* Once we get the FEN positions, we can derive the values for the input data we wish to parse into our model\n",
    "\n",
    "##### Data Representation\n",
    "* Although this doesn't give the chess positions directly, we can manipulate it into a form that works for the VAE.\n",
    "* The current idea is to have a 8 x 8 x 12 matrix, which means to say each of the 12 pieces (K, Q, R, B, N, P, k, q, r, b, n, p) each have their own 8 x 8 chessboard that denotes their position.\n",
    "* We can generate these as all chess games I exported start from the standard position, and we can denote the piece at a certain position with a 1 (i.e. 0 marks that the piece is not at that position).\n",
    "* This coincidentally is a perfect data set for generating anti-puzzles as it is formed from the sequence of moves of a game, of which not all positions have an optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = os.path.dirname(__vsc_ipynb_file__)\n",
    "fen_data_path = os.path.join(DIR, \"data\", \"fen-data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIECE_TO_IDX = dict([[c, i] for i, c in enumerate('KQRBNPkqrbnp')])\n",
    "\n",
    "def generate_matrix_from_fen(fen_string):\n",
    "    # initialise board\n",
    "    board = [[[0 for k in range(12)] for j in range(8)] for i in range(8)]\n",
    "\n",
    "    # process FEN string\n",
    "    board_string = fen_string.split(\" \")[0].split(\"/\")\n",
    "    row, col = 0, 0\n",
    "    for board_row in board_string:\n",
    "        for row_item in board_row:\n",
    "            if row_item.isnumeric():\n",
    "                col += int(row_item)\n",
    "            else:\n",
    "                board[row][col][PIECE_TO_IDX[row_item]] = 1\n",
    "                col += 1\n",
    "        row += 1\n",
    "        col = 0\n",
    "    return board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18175, 8, 8, 12)\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "with open(fen_data_path) as file:\n",
    "    for line in file:\n",
    "        data.append(np.array(generate_matrix_from_fen(line)))\n",
    "data = np.array(data, 'float64')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (hyper)parameters\n",
    "latent_dims = 8\n",
    "hidden_layers = 3\n",
    "base_units = 2 << 5\n",
    "kernel_size = (3, 3)\n",
    "strides = 2\n",
    "dropout_rate = 0.3\n",
    "beta = 10 ** -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 8, 8, 12)]   0           []                               \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 4, 4, 64)     6976        ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 4, 4, 64)    256         ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 4, 4, 64)     0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)           (None, 4, 4, 64)     0           ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 2, 2, 128)    73856       ['dropout_36[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 2, 2, 128)   512         ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 2, 2, 128)    0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 2, 2, 128)    0           ['activation_37[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 1, 1, 256)    295168      ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 1, 1, 256)   1024        ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 1, 1, 256)    0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 1, 1, 256)    0           ['activation_38[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_6 (Flatten)            (None, 256)          0           ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 8)            2056        ['flatten_6[0][0]']              \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 8)            2056        ['flatten_6[0][0]']              \n",
      "                                                                                                  \n",
      " sampling_6 (Sampling)          (None, 8)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 381,904\n",
      "Trainable params: 381,008\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               2304      \n",
      "                                                                 \n",
      " reshape_6 (Reshape)         (None, 1, 1, 256)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_24 (Conv2D  (None, 2, 2, 256)        590080    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_39 (Bat  (None, 2, 2, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_39 (Activation)  (None, 2, 2, 256)         0         \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 2, 2, 256)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_25 (Conv2D  (None, 4, 4, 128)        295040    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_40 (Bat  (None, 4, 4, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_40 (Activation)  (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_26 (Conv2D  (None, 8, 8, 64)         73792     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_41 (Bat  (None, 8, 8, 64)         256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_41 (Activation)  (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_transpose_27 (Conv2D  (None, 8, 8, 12)         6924      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 969,932\n",
      "Trainable params: 969,036\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape = (batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class vae_chess(models.Model):\n",
    "\n",
    "    def __init__(self, latent_dims, hidden_layers, base_units, kernel_size, strides, dropout_rate, beta):\n",
    "        super(vae_chess, self).__init__()\n",
    "\n",
    "        self.latent_dims = latent_dims\n",
    "        self.beta = beta\n",
    "\n",
    "        self.encoder = self.generate_encoder_model(hidden_layers, base_units, kernel_size, strides, dropout_rate)\n",
    "        self.decoder = self.generate_decoder_model(hidden_layers, base_units, kernel_size, strides, dropout_rate)\n",
    "        print(self.encoder.summary())\n",
    "        print(self.decoder.summary())\n",
    "\n",
    "        self.total_loss_tracker = metrics.Mean(name = \"total_loss\")\n",
    "        self.reconstruction_loss_tracker = metrics.Mean(name = \"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = metrics.Mean(name = \"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.total_loss_tracker, self.reconstruction_loss_tracker, self.kl_loss_tracker]\n",
    "\n",
    "    def generate_encoder_model(self, hidden_layers, base_units, kernel_size, strides, dropout_rate):\n",
    "        encoder_input = layers.Input(shape = (8, 8, 12), name = \"encoder_input\")\n",
    "\n",
    "        for i in range(hidden_layers):\n",
    "            conv_layer = layers.Conv2D(base_units << i, kernel_size, strides, padding = \"same\")(encoder_input if i == 0 else dropout_layer)\n",
    "            batch_norm_layer = layers.BatchNormalization()(conv_layer)\n",
    "            activation_layer = layers.Activation('relu')(batch_norm_layer)\n",
    "            dropout_layer = layers.Dropout(dropout_rate)(activation_layer)\n",
    "        self.pass_back_shape = K.int_shape(dropout_layer)[1:]\n",
    "\n",
    "        flatten_layer = layers.Flatten()(dropout_layer)\n",
    "        z_mean = layers.Dense(self.latent_dims, name = \"z_mean\")(flatten_layer)\n",
    "        z_log_var = layers.Dense(self.latent_dims, name = \"z_log_var\")(flatten_layer)\n",
    "        z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "        return models.Model(encoder_input, [z_mean, z_log_var, z], name = \"encoder\")\n",
    "    \n",
    "    def generate_decoder_model(self, hidden_layers, base_units, kernel_size, strides, dropout_rate):\n",
    "        decoder_input = layers.Input(shape = (self.latent_dims), name = \"decoder_input\")\n",
    "\n",
    "        before_reshape = layers.Dense(np.prod(self.pass_back_shape))(decoder_input)\n",
    "        reshape_layer = layers.Reshape(self.pass_back_shape)(before_reshape)\n",
    "\n",
    "        for i in range(hidden_layers - 1, -1, -1):\n",
    "            conv_transpose_layer = layers.Conv2DTranspose(base_units << i, kernel_size, strides, padding = \"same\")(reshape_layer if i == hidden_layers - 1 else dropout_layer)\n",
    "            batch_norm_layer = layers.BatchNormalization()(conv_transpose_layer)\n",
    "            activation_layer = layers.Activation('relu')(batch_norm_layer)\n",
    "            dropout_layer = layers.Dropout(dropout_rate)(activation_layer)\n",
    "\n",
    "        decoder_output = layers.Conv2DTranspose(12, kernel_size, 1, padding = \"same\")(dropout_layer)\n",
    "\n",
    "        return models.Model(decoder_input, decoder_output, name = \"decoder\")\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return z_mean, z_log_var, reconstruction\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, reconstruction = self(data)\n",
    "            reconstruction_loss = tf.reduce_mean(losses.binary_crossentropy(data, reconstruction, axis = (1, 2, 3)))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(-0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)), axis = 1))\n",
    "            total_loss = reconstruction_loss + self.beta * kl_loss\n",
    "        \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {m.name : m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        z_mean, z_log_var, reconstruction = self(data)\n",
    "        reconstruction_loss = tf.reduce_mean(losses.binary_crossentropy(data, reconstruction, axis = (1, 2, 3)))\n",
    "        kl_loss = tf.reduce_mean(tf.reduce_sum(-0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)), axis = 1))\n",
    "        total_loss = reconstruction_loss + self.beta * kl_loss\n",
    "        \n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {m.name : m.result() for m in self.metrics}\n",
    "\n",
    "vae = vae_chess(latent_dims, hidden_layers, base_units, kernel_size, strides, dropout_rate, beta)\n",
    "vae.compile(optimizer = \"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "64/64 [==============================] - 3s 48ms/step - total_loss: 29.4790 - reconstruction_loss: 29.4404 - kl_loss: 0.3867 - val_total_loss: 0.1019 - val_reconstruction_loss: 0.1017 - val_kl_loss: 0.0022\n",
      "Epoch 2/500\n",
      "64/64 [==============================] - 3s 47ms/step - total_loss: 28.7253 - reconstruction_loss: 28.6597 - kl_loss: 0.6557 - val_total_loss: 0.0993 - val_reconstruction_loss: 0.0971 - val_kl_loss: 0.0215\n",
      "Epoch 3/500\n",
      "64/64 [==============================] - 3s 47ms/step - total_loss: 27.4941 - reconstruction_loss: 27.3852 - kl_loss: 1.0892 - val_total_loss: 0.0834 - val_reconstruction_loss: 0.0834 - val_kl_loss: 2.5326e-04\n",
      "Epoch 4/500\n",
      "64/64 [==============================] - 3s 48ms/step - total_loss: 25.7541 - reconstruction_loss: 25.7152 - kl_loss: 0.3887 - val_total_loss: 0.1047 - val_reconstruction_loss: 0.1046 - val_kl_loss: 7.2648e-04\n",
      "Epoch 5/500\n",
      "64/64 [==============================] - 3s 47ms/step - total_loss: 27.1229 - reconstruction_loss: 27.0318 - kl_loss: 0.9112 - val_total_loss: 0.1211 - val_reconstruction_loss: 0.1207 - val_kl_loss: 0.0041\n",
      "Epoch 6/500\n",
      "64/64 [==============================] - 3s 47ms/step - total_loss: 25.8125 - reconstruction_loss: 25.7617 - kl_loss: 0.5086 - val_total_loss: 0.0984 - val_reconstruction_loss: 0.0983 - val_kl_loss: 0.0013\n",
      "Epoch 7/500\n",
      "64/64 [==============================] - 3s 47ms/step - total_loss: 25.2145 - reconstruction_loss: 25.1742 - kl_loss: 0.4029 - val_total_loss: 0.0965 - val_reconstruction_loss: 0.0964 - val_kl_loss: 5.1127e-04\n",
      "Epoch 8/500\n",
      "64/64 [==============================] - 3s 47ms/step - total_loss: 24.9917 - reconstruction_loss: 24.9442 - kl_loss: 0.4746 - val_total_loss: 0.0796 - val_reconstruction_loss: 0.0796 - val_kl_loss: 1.9959e-04\n",
      "Epoch 9/500\n",
      "64/64 [==============================] - 3s 47ms/step - total_loss: 24.6269 - reconstruction_loss: 24.5895 - kl_loss: 0.3738 - val_total_loss: 0.0967 - val_reconstruction_loss: 0.0966 - val_kl_loss: 8.2883e-04\n",
      "Epoch 10/500\n",
      "64/64 [==============================] - 3s 47ms/step - total_loss: 24.3990 - reconstruction_loss: 24.3582 - kl_loss: 0.4080 - val_total_loss: 0.1030 - val_reconstruction_loss: 0.1030 - val_kl_loss: 1.6373e-04\n",
      "Epoch 11/500\n",
      "64/64 [==============================] - 3s 47ms/step - total_loss: 24.1538 - reconstruction_loss: 24.1157 - kl_loss: 0.3806 - val_total_loss: 0.0950 - val_reconstruction_loss: 0.0950 - val_kl_loss: 1.9895e-04\n",
      "Epoch 12/500\n",
      "64/64 [==============================] - 3s 48ms/step - total_loss: 23.9052 - reconstruction_loss: 23.8706 - kl_loss: 0.3466 - val_total_loss: 0.0987 - val_reconstruction_loss: 0.0987 - val_kl_loss: 5.5203e-04\n",
      "Epoch 13/500\n",
      "64/64 [==============================] - 3s 48ms/step - total_loss: 23.8011 - reconstruction_loss: 23.7674 - kl_loss: 0.3365 - val_total_loss: 0.0989 - val_reconstruction_loss: 0.0989 - val_kl_loss: 1.2255e-04\n",
      "Epoch 14/500\n",
      "64/64 [==============================] - 3s 49ms/step - total_loss: 23.5167 - reconstruction_loss: 23.4883 - kl_loss: 0.2834 - val_total_loss: 0.0961 - val_reconstruction_loss: 0.0960 - val_kl_loss: 6.6191e-04\n",
      "Epoch 15/500\n",
      "64/64 [==============================] - 3s 49ms/step - total_loss: 24.3920 - reconstruction_loss: 24.3400 - kl_loss: 0.5197 - val_total_loss: 0.0989 - val_reconstruction_loss: 0.0988 - val_kl_loss: 7.0231e-04\n",
      "Epoch 16/500\n",
      "64/64 [==============================] - 3s 49ms/step - total_loss: 24.5121 - reconstruction_loss: 24.4669 - kl_loss: 0.4519 - val_total_loss: 0.0977 - val_reconstruction_loss: 0.0975 - val_kl_loss: 0.0025\n",
      "Epoch 17/500\n",
      "64/64 [==============================] - 3s 48ms/step - total_loss: 25.0272 - reconstruction_loss: 24.9680 - kl_loss: 0.5915 - val_total_loss: 0.0958 - val_reconstruction_loss: 0.0957 - val_kl_loss: 7.2867e-04\n",
      "Epoch 18/500\n",
      "64/64 [==============================] - 3s 47ms/step - total_loss: 24.0020 - reconstruction_loss: 23.9699 - kl_loss: 0.3207 - val_total_loss: 0.0986 - val_reconstruction_loss: 0.0986 - val_kl_loss: 3.1899e-04\n",
      "Epoch 19/500\n",
      "64/64 [==============================] - 3s 49ms/step - total_loss: 23.5054 - reconstruction_loss: 23.4806 - kl_loss: 0.2478 - val_total_loss: 0.0944 - val_reconstruction_loss: 0.0944 - val_kl_loss: 3.9952e-04\n",
      "Epoch 20/500\n",
      "64/64 [==============================] - 3s 49ms/step - total_loss: 23.3550 - reconstruction_loss: 23.3324 - kl_loss: 0.2257 - val_total_loss: 0.0968 - val_reconstruction_loss: 0.0968 - val_kl_loss: 4.8235e-05\n",
      "Epoch 21/500\n",
      "64/64 [==============================] - 3s 50ms/step - total_loss: 23.4758 - reconstruction_loss: 23.4585 - kl_loss: 0.1731 - val_total_loss: 0.0954 - val_reconstruction_loss: 0.0954 - val_kl_loss: 9.6247e-05\n",
      "Epoch 22/500\n",
      "64/64 [==============================] - 4s 56ms/step - total_loss: 23.0784 - reconstruction_loss: 23.0668 - kl_loss: 0.1161 - val_total_loss: 0.0943 - val_reconstruction_loss: 0.0943 - val_kl_loss: 8.2523e-05\n",
      "Epoch 23/500\n",
      "64/64 [==============================] - 3s 51ms/step - total_loss: 23.7293 - reconstruction_loss: 23.7014 - kl_loss: 0.2790 - val_total_loss: 0.1104 - val_reconstruction_loss: 0.1104 - val_kl_loss: 5.2461e-04\n",
      "Epoch 24/500\n",
      "64/64 [==============================] - 4s 55ms/step - total_loss: 23.5667 - reconstruction_loss: 23.5549 - kl_loss: 0.1182 - val_total_loss: 0.0977 - val_reconstruction_loss: 0.0977 - val_kl_loss: 2.5821e-04\n",
      "Epoch 25/500\n",
      "61/64 [===========================>..] - ETA: 0s - total_loss: 23.9999 - reconstruction_loss: 23.9622 - kl_loss: 0.3766"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m EPOCHS \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m\n\u001b[1;32m      2\u001b[0m BATCH_SIZE \u001b[39m=\u001b[39m \u001b[39m256\u001b[39m\n\u001b[0;32m----> 4\u001b[0m vae\u001b[39m.\u001b[39;49mfit(data, epochs \u001b[39m=\u001b[39;49m EPOCHS, batch_size \u001b[39m=\u001b[39;49m BATCH_SIZE, shuffle \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m, validation_split \u001b[39m=\u001b[39;49m \u001b[39m0.1\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/python311/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/python311/lib/python3.11/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/python311/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/python311/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/python311/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/python311/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/python311/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/python311/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/python311/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 500\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "vae.fit(data, epochs = EPOCHS, batch_size = BATCH_SIZE, shuffle = True, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
