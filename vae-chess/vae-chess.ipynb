{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational AutoEncoder Chess Position Generator\n",
    "\n",
    "##### Inspiration\n",
    "* Recently, I have been reading up about generative models, and one of them that caught my eye was the VAE.\n",
    "* It allows you to generate new data that is similar to your training data.\n",
    "* At the same time, I am interested in chess and have enjoyed solving chess puzzles for quite awhile.\n",
    "* However, the premise of a chess puzzle is that the player knows that there exists a optimal move / sequence of moves that provides the player an advantage.\n",
    "* This helps the player to improve in terms of tactics and pattern recognition, but in most cases when playing a game of chess, we do not know if there exists an optimal solution.\n",
    "* This introduces the idea of an anti-puzzle, where the premise is now that the chess position provided may have an optimal solution, or the \"solution\" is to play a move that maintains the status-quo.\n",
    "* With the VAE, we can train it with a training set of legal chess positions, and have it output more chess positions.\n",
    "* Since the VAE would not have any idea if the chess position has an optimal solution or not, it is perfect for creating \"anti-puzzle\" solutions.\n",
    "* Furthermore, chess is a \"constrained\" game, where the rules are clear and we can check if the position generated by the VAE is a legal position or not.\n",
    "* For this model, the goal is to simply generate new (legal) chess positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pprint\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from scipy.stats import norm\n",
    "from keras import layers, models, metrics, losses, optimizers, activations\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Collection\n",
    "\n",
    "* The easiest way to obtain chess is positions is from my own games.\n",
    "* I exported move data from some chess games that I have played online in Lichess, which comes in a .pgn file.\n",
    "* From this file, we can get the move orders for the games that I have exported, from which I can deduce the chess positions.\n",
    "* For this, I used the python-chess library, which helps to deduce FEN positions from PGN move list\n",
    "* Once we get the FEN positions, we can derive the values for the input data we wish to parse into our model\n",
    "\n",
    "##### Data Representation\n",
    "* Although this doesn't give the chess positions directly, we can manipulate it into a form that works for the VAE.\n",
    "* The current idea is to have a 8 x 8 x 12 matrix, which means to say each of the 12 pieces (K, Q, R, B, N, P, k, q, r, b, n, p) each have their own 8 x 8 chessboard that denotes their position.\n",
    "* We can generate these as all chess games I exported start from the standard position, and we can denote the piece at a certain position with a 1 (i.e. 0 marks that the piece is not at that position).\n",
    "* This coincidentally is a perfect data set for generating anti-puzzles as it is formed from the sequence of moves of a game, of which not all positions have an optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = os.path.dirname(__vsc_ipynb_file__)\n",
    "fen_data_path = os.path.join(DIR, \"data\", \"fen-data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIECE_TO_IDX = dict([[c, i] for i, c in enumerate('KQRBNPkqrbnp')])\n",
    "\n",
    "def generate_matrix_from_fen(fen_string):\n",
    "    # initialise board\n",
    "    board = [[[0 for k in range(12)] for j in range(8)] for i in range(8)]\n",
    "\n",
    "    # process FEN string\n",
    "    board_string = fen_string.split(\" \")[0].split(\"/\")\n",
    "    row, col = 0, 0\n",
    "    for board_row in board_string:\n",
    "        for row_item in board_row:\n",
    "            if row_item.isnumeric():\n",
    "                col += int(row_item)\n",
    "            else:\n",
    "                board[row][col][PIECE_TO_IDX[row_item]] = 1\n",
    "                col += 1\n",
    "        row += 1\n",
    "        col = 0\n",
    "    return board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30920, 8, 8, 12)\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "with open(fen_data_path) as file:\n",
    "    for line in file:\n",
    "        data.append(np.array(generate_matrix_from_fen(line)))\n",
    "data = np.array(data, 'float64')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (hyper)parameters\n",
    "latent_dims = 8\n",
    "hidden_layers = 3\n",
    "base_units = 2 << 3\n",
    "kernel_size = (2, 2)\n",
    "strides = 2\n",
    "dropout_rate = 0.3\n",
    "threshold = 0.3\n",
    "beta_1 = 10 ** 4\n",
    "beta_2 = 10 ** -2\n",
    "learning_rate = 10 ** -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 8, 8, 12)]   0           []                               \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)             (None, 4, 4, 16)     784         ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_144 (Batch  (None, 4, 4, 16)    64          ['conv2d_72[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_144 (Activation)    (None, 4, 4, 16)     0           ['batch_normalization_144[0][0]']\n",
      "                                                                                                  \n",
      " dropout_144 (Dropout)          (None, 4, 4, 16)     0           ['activation_144[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)             (None, 2, 2, 32)     2080        ['dropout_144[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_145 (Batch  (None, 2, 2, 32)    128         ['conv2d_73[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_145 (Activation)    (None, 2, 2, 32)     0           ['batch_normalization_145[0][0]']\n",
      "                                                                                                  \n",
      " dropout_145 (Dropout)          (None, 2, 2, 32)     0           ['activation_145[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)             (None, 1, 1, 64)     8256        ['dropout_145[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_146 (Batch  (None, 1, 1, 64)    256         ['conv2d_74[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_146 (Activation)    (None, 1, 1, 64)     0           ['batch_normalization_146[0][0]']\n",
      "                                                                                                  \n",
      " dropout_146 (Dropout)          (None, 1, 1, 64)     0           ['activation_146[0][0]']         \n",
      "                                                                                                  \n",
      " flatten_24 (Flatten)           (None, 64)           0           ['dropout_146[0][0]']            \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 8)            520         ['flatten_24[0][0]']             \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 8)            520         ['flatten_24[0][0]']             \n",
      "                                                                                                  \n",
      " sampling_24 (Sampling)         (None, 8)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,608\n",
      "Trainable params: 12,384\n",
      "Non-trainable params: 224\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " reshape_24 (Reshape)        (None, 1, 1, 64)          0         \n",
      "                                                                 \n",
      " conv2d_transpose_96 (Conv2D  (None, 2, 2, 64)         16448     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_147 (Ba  (None, 2, 2, 64)         256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_147 (Activation)  (None, 2, 2, 64)         0         \n",
      "                                                                 \n",
      " dropout_147 (Dropout)       (None, 2, 2, 64)          0         \n",
      "                                                                 \n",
      " conv2d_transpose_97 (Conv2D  (None, 4, 4, 32)         8224      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_148 (Ba  (None, 4, 4, 32)         128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_148 (Activation)  (None, 4, 4, 32)         0         \n",
      "                                                                 \n",
      " dropout_148 (Dropout)       (None, 4, 4, 32)          0         \n",
      "                                                                 \n",
      " conv2d_transpose_98 (Conv2D  (None, 8, 8, 16)         2064      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_149 (Ba  (None, 8, 8, 16)         64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_149 (Activation)  (None, 8, 8, 16)         0         \n",
      "                                                                 \n",
      " dropout_149 (Dropout)       (None, 8, 8, 16)          0         \n",
      "                                                                 \n",
      " conv2d_transpose_99 (Conv2D  (None, 8, 8, 12)         780       \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " tf.math.tanh_4 (TFOpLambda)  (None, 8, 8, 12)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,540\n",
      "Trainable params: 28,316\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape = (batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class vae_chess(models.Model):\n",
    "\n",
    "    def __init__(self, latent_dims, hidden_layers, base_units, kernel_size, strides, dropout_rate, threshold, beta_1, beta_2):\n",
    "        super(vae_chess, self).__init__()\n",
    "\n",
    "        self.latent_dims = latent_dims\n",
    "        self.threshold = threshold\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "        self.encoder = self.generate_encoder_model(hidden_layers, base_units, kernel_size, strides, dropout_rate)\n",
    "        self.decoder = self.generate_decoder_model(hidden_layers, base_units, kernel_size, strides, dropout_rate)\n",
    "        print(self.encoder.summary())\n",
    "        print(self.decoder.summary())\n",
    "\n",
    "        self.total_loss_tracker = metrics.Mean(name = \"total_loss\")\n",
    "        self.reconstruction_loss_tracker = metrics.Mean(name = \"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = metrics.Mean(name = \"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.total_loss_tracker, self.reconstruction_loss_tracker, self.kl_loss_tracker]\n",
    "\n",
    "    def generate_encoder_model(self, hidden_layers, base_units, kernel_size, strides, dropout_rate):\n",
    "        encoder_input = layers.Input(shape = (8, 8, 12), name = \"encoder_input\")\n",
    "\n",
    "        for i in range(hidden_layers):\n",
    "            conv_layer = layers.Conv2D(base_units << i, kernel_size, strides, padding = \"same\")(encoder_input if i == 0 else dropout_layer)\n",
    "            batch_norm_layer = layers.BatchNormalization()(conv_layer)\n",
    "            activation_layer = layers.Activation('relu')(batch_norm_layer)\n",
    "            dropout_layer = layers.Dropout(dropout_rate)(activation_layer)\n",
    "        self.pass_back_shape = K.int_shape(dropout_layer)[1:]\n",
    "\n",
    "        flatten_layer = layers.Flatten()(dropout_layer)\n",
    "        z_mean = layers.Dense(self.latent_dims, name = \"z_mean\")(flatten_layer)\n",
    "        z_log_var = layers.Dense(self.latent_dims, name = \"z_log_var\")(flatten_layer)\n",
    "        z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "        return models.Model(encoder_input, [z_mean, z_log_var, z], name = \"encoder\")\n",
    "    \n",
    "    def generate_decoder_model(self, hidden_layers, base_units, kernel_size, strides, dropout_rate):\n",
    "        decoder_input = layers.Input(shape = (self.latent_dims), name = \"decoder_input\")\n",
    "\n",
    "        before_reshape = layers.Dense(np.prod(self.pass_back_shape))(decoder_input)\n",
    "        reshape_layer = layers.Reshape(self.pass_back_shape)(before_reshape)\n",
    "\n",
    "        for i in range(hidden_layers - 1, -1, -1):\n",
    "            conv_transpose_layer = layers.Conv2DTranspose(base_units << i, kernel_size, strides, padding = \"same\")(reshape_layer if i == hidden_layers - 1 else dropout_layer)\n",
    "            batch_norm_layer = layers.BatchNormalization()(conv_transpose_layer)\n",
    "            activation_layer = layers.Activation('relu')(batch_norm_layer)\n",
    "            dropout_layer = layers.Dropout(dropout_rate)(activation_layer)\n",
    "\n",
    "        decoder_output = layers.Conv2DTranspose(12, kernel_size, 1, padding = \"same\")(dropout_layer)\n",
    "        decoder_output_transformed = activations.tanh(decoder_output)\n",
    "\n",
    "        return models.Model(decoder_input, decoder_output_transformed, name = \"decoder\")\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return z_mean, z_log_var, reconstruction\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, reconstruction = self(data)\n",
    "            reconstruction_loss = tf.reduce_mean(losses.binary_crossentropy(data, reconstruction, axis = (1, 2, 3)))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(-0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)), axis = 1))\n",
    "            total_loss = self.beta_1 * reconstruction_loss + self.beta_2 * kl_loss\n",
    "        \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {m.name : m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        z_mean, z_log_var, reconstruction = self(data)\n",
    "        reconstruction_loss = tf.reduce_mean(losses.binary_crossentropy(data, reconstruction, axis = (1, 2, 3)))\n",
    "        kl_loss = tf.reduce_mean(tf.reduce_sum(-0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)), axis = 1))\n",
    "        total_loss = self.beta_1 * reconstruction_loss + self.beta_2 * kl_loss\n",
    "        \n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {m.name : m.result() for m in self.metrics}\n",
    "    \n",
    "    def generate_sample(self, num_samples):\n",
    "        z_sample = tf.random.normal(shape = (num_samples, self.latent_dims))\n",
    "        return self.decoder(z_sample)\n",
    "\n",
    "vae = vae_chess(latent_dims, hidden_layers, base_units, kernel_size, strides, dropout_rate, threshold, beta_1, beta_2)\n",
    "optimiser = optimizers.Adam(learning_rate = learning_rate)\n",
    "vae.compile(optimizer = \"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "109/109 [==============================] - 4s 13ms/step - total_loss: 1391.7561 - reconstruction_loss: 0.1392 - kl_loss: 0.1468 - val_total_loss: 1177.1415 - val_reconstruction_loss: 0.1177 - val_kl_loss: 0.5436\n",
      "Epoch 2/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 1102.2075 - reconstruction_loss: 0.1102 - kl_loss: 7.2891 - val_total_loss: 1010.2437 - val_reconstruction_loss: 0.1010 - val_kl_loss: 11.0450\n",
      "Epoch 3/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 973.8298 - reconstruction_loss: 0.0974 - kl_loss: 19.9219 - val_total_loss: 939.2612 - val_reconstruction_loss: 0.0939 - val_kl_loss: 26.0851\n",
      "Epoch 4/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 891.6949 - reconstruction_loss: 0.0891 - kl_loss: 25.8887 - val_total_loss: 857.6429 - val_reconstruction_loss: 0.0857 - val_kl_loss: 25.0839\n",
      "Epoch 5/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 842.1235 - reconstruction_loss: 0.0842 - kl_loss: 30.2236 - val_total_loss: 915.2073 - val_reconstruction_loss: 0.0915 - val_kl_loss: 17.3953\n",
      "Epoch 6/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 862.5167 - reconstruction_loss: 0.0862 - kl_loss: 23.7790 - val_total_loss: 825.2994 - val_reconstruction_loss: 0.0825 - val_kl_loss: 21.2135\n",
      "Epoch 7/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 830.2551 - reconstruction_loss: 0.0830 - kl_loss: 23.2780 - val_total_loss: 811.5623 - val_reconstruction_loss: 0.0811 - val_kl_loss: 25.0416\n",
      "Epoch 8/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 841.2444 - reconstruction_loss: 0.0841 - kl_loss: 25.2288 - val_total_loss: 814.3090 - val_reconstruction_loss: 0.0814 - val_kl_loss: 23.7760\n",
      "Epoch 9/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 816.4849 - reconstruction_loss: 0.0816 - kl_loss: 23.5767 - val_total_loss: 791.4056 - val_reconstruction_loss: 0.0791 - val_kl_loss: 24.8129\n",
      "Epoch 10/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 886.3177 - reconstruction_loss: 0.0886 - kl_loss: 28.9318 - val_total_loss: 970.3597 - val_reconstruction_loss: 0.0970 - val_kl_loss: 30.1501\n",
      "Epoch 11/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 919.6742 - reconstruction_loss: 0.0919 - kl_loss: 27.3253 - val_total_loss: 858.6036 - val_reconstruction_loss: 0.0858 - val_kl_loss: 25.0203\n",
      "Epoch 12/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 838.6183 - reconstruction_loss: 0.0838 - kl_loss: 28.3723 - val_total_loss: 819.1023 - val_reconstruction_loss: 0.0819 - val_kl_loss: 28.0294\n",
      "Epoch 13/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 874.6975 - reconstruction_loss: 0.0874 - kl_loss: 21.9105 - val_total_loss: 889.5909 - val_reconstruction_loss: 0.0889 - val_kl_loss: 15.6942\n",
      "Epoch 14/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 860.9678 - reconstruction_loss: 0.0861 - kl_loss: 17.3731 - val_total_loss: 844.8954 - val_reconstruction_loss: 0.0845 - val_kl_loss: 17.7692\n",
      "Epoch 15/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 843.8478 - reconstruction_loss: 0.0844 - kl_loss: 18.0892 - val_total_loss: 853.8814 - val_reconstruction_loss: 0.0854 - val_kl_loss: 16.5168\n",
      "Epoch 16/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 814.3803 - reconstruction_loss: 0.0814 - kl_loss: 18.3796 - val_total_loss: 798.3118 - val_reconstruction_loss: 0.0798 - val_kl_loss: 17.9949\n",
      "Epoch 17/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 803.2025 - reconstruction_loss: 0.0803 - kl_loss: 18.9653 - val_total_loss: 800.3000 - val_reconstruction_loss: 0.0800 - val_kl_loss: 18.2110\n",
      "Epoch 18/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 792.1998 - reconstruction_loss: 0.0792 - kl_loss: 21.4420 - val_total_loss: 777.3904 - val_reconstruction_loss: 0.0777 - val_kl_loss: 22.3271\n",
      "Epoch 19/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 789.4278 - reconstruction_loss: 0.0789 - kl_loss: 24.3461 - val_total_loss: 785.7548 - val_reconstruction_loss: 0.0785 - val_kl_loss: 26.1564\n",
      "Epoch 20/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 775.4131 - reconstruction_loss: 0.0775 - kl_loss: 25.6351 - val_total_loss: 756.0469 - val_reconstruction_loss: 0.0756 - val_kl_loss: 23.2130\n",
      "Epoch 21/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 804.2945 - reconstruction_loss: 0.0804 - kl_loss: 28.3726 - val_total_loss: 769.0795 - val_reconstruction_loss: 0.0769 - val_kl_loss: 29.3369\n",
      "Epoch 22/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 773.4305 - reconstruction_loss: 0.0773 - kl_loss: 29.2347 - val_total_loss: 778.7968 - val_reconstruction_loss: 0.0779 - val_kl_loss: 23.7539\n",
      "Epoch 23/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 789.0461 - reconstruction_loss: 0.0789 - kl_loss: 25.1375 - val_total_loss: 759.7231 - val_reconstruction_loss: 0.0759 - val_kl_loss: 24.7755\n",
      "Epoch 24/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 760.7216 - reconstruction_loss: 0.0760 - kl_loss: 26.5763 - val_total_loss: 770.8607 - val_reconstruction_loss: 0.0771 - val_kl_loss: 23.3483\n",
      "Epoch 25/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 754.2678 - reconstruction_loss: 0.0754 - kl_loss: 26.6744 - val_total_loss: 747.9691 - val_reconstruction_loss: 0.0748 - val_kl_loss: 26.5615\n",
      "Epoch 26/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 754.9339 - reconstruction_loss: 0.0755 - kl_loss: 27.2243 - val_total_loss: 806.4424 - val_reconstruction_loss: 0.0806 - val_kl_loss: 21.7231\n",
      "Epoch 27/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 775.3472 - reconstruction_loss: 0.0775 - kl_loss: 25.6773 - val_total_loss: 757.9932 - val_reconstruction_loss: 0.0758 - val_kl_loss: 22.9170\n",
      "Epoch 28/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 756.6909 - reconstruction_loss: 0.0756 - kl_loss: 25.9270 - val_total_loss: 734.4036 - val_reconstruction_loss: 0.0734 - val_kl_loss: 26.1330\n",
      "Epoch 29/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 740.3686 - reconstruction_loss: 0.0740 - kl_loss: 27.3956 - val_total_loss: 736.0110 - val_reconstruction_loss: 0.0736 - val_kl_loss: 25.9558\n",
      "Epoch 30/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 737.7302 - reconstruction_loss: 0.0737 - kl_loss: 27.7601 - val_total_loss: 724.3799 - val_reconstruction_loss: 0.0724 - val_kl_loss: 26.7459\n",
      "Epoch 31/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 722.2250 - reconstruction_loss: 0.0722 - kl_loss: 29.2204 - val_total_loss: 716.5582 - val_reconstruction_loss: 0.0716 - val_kl_loss: 28.8078\n",
      "Epoch 32/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 721.4711 - reconstruction_loss: 0.0721 - kl_loss: 30.2080 - val_total_loss: 715.5388 - val_reconstruction_loss: 0.0715 - val_kl_loss: 28.8627\n",
      "Epoch 33/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 725.2642 - reconstruction_loss: 0.0725 - kl_loss: 29.5212 - val_total_loss: 767.4464 - val_reconstruction_loss: 0.0767 - val_kl_loss: 29.0006\n",
      "Epoch 34/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 730.0009 - reconstruction_loss: 0.0730 - kl_loss: 32.1893 - val_total_loss: 721.6283 - val_reconstruction_loss: 0.0721 - val_kl_loss: 29.4196\n",
      "Epoch 35/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 720.0913 - reconstruction_loss: 0.0720 - kl_loss: 32.1279 - val_total_loss: 707.6907 - val_reconstruction_loss: 0.0707 - val_kl_loss: 31.4323\n",
      "Epoch 36/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 733.3329 - reconstruction_loss: 0.0733 - kl_loss: 33.2976 - val_total_loss: 864.3300 - val_reconstruction_loss: 0.0864 - val_kl_loss: 30.2389\n",
      "Epoch 37/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 801.6958 - reconstruction_loss: 0.0801 - kl_loss: 28.8302 - val_total_loss: 900.3343 - val_reconstruction_loss: 0.0900 - val_kl_loss: 16.1335\n",
      "Epoch 38/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 783.8071 - reconstruction_loss: 0.0784 - kl_loss: 19.8998 - val_total_loss: 735.8307 - val_reconstruction_loss: 0.0736 - val_kl_loss: 21.0534\n",
      "Epoch 39/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 740.0602 - reconstruction_loss: 0.0740 - kl_loss: 23.4914 - val_total_loss: 720.9421 - val_reconstruction_loss: 0.0721 - val_kl_loss: 23.4664\n",
      "Epoch 40/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 715.0439 - reconstruction_loss: 0.0715 - kl_loss: 26.9049 - val_total_loss: 707.8538 - val_reconstruction_loss: 0.0708 - val_kl_loss: 25.9456\n",
      "Epoch 41/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 740.1550 - reconstruction_loss: 0.0740 - kl_loss: 30.7126 - val_total_loss: 744.8129 - val_reconstruction_loss: 0.0745 - val_kl_loss: 26.2283\n",
      "Epoch 42/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 722.8243 - reconstruction_loss: 0.0723 - kl_loss: 28.8057 - val_total_loss: 706.9284 - val_reconstruction_loss: 0.0707 - val_kl_loss: 28.8046\n",
      "Epoch 43/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 704.0731 - reconstruction_loss: 0.0704 - kl_loss: 30.5809 - val_total_loss: 702.8422 - val_reconstruction_loss: 0.0703 - val_kl_loss: 29.6043\n",
      "Epoch 44/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 711.4222 - reconstruction_loss: 0.0711 - kl_loss: 30.5506 - val_total_loss: 715.5419 - val_reconstruction_loss: 0.0715 - val_kl_loss: 28.9790\n",
      "Epoch 45/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 706.5179 - reconstruction_loss: 0.0706 - kl_loss: 30.9079 - val_total_loss: 706.2281 - val_reconstruction_loss: 0.0706 - val_kl_loss: 29.3779\n",
      "Epoch 46/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 746.1187 - reconstruction_loss: 0.0746 - kl_loss: 25.3920 - val_total_loss: 719.3856 - val_reconstruction_loss: 0.0719 - val_kl_loss: 24.5868\n",
      "Epoch 47/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 736.1227 - reconstruction_loss: 0.0736 - kl_loss: 23.7764 - val_total_loss: 755.9895 - val_reconstruction_loss: 0.0756 - val_kl_loss: 18.9852\n",
      "Epoch 48/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 747.6464 - reconstruction_loss: 0.0747 - kl_loss: 21.3781 - val_total_loss: 769.6928 - val_reconstruction_loss: 0.0770 - val_kl_loss: 17.1203\n",
      "Epoch 49/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 770.0460 - reconstruction_loss: 0.0770 - kl_loss: 17.8025 - val_total_loss: 736.0572 - val_reconstruction_loss: 0.0736 - val_kl_loss: 16.3568\n",
      "Epoch 50/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 750.3630 - reconstruction_loss: 0.0750 - kl_loss: 19.3183 - val_total_loss: 733.2302 - val_reconstruction_loss: 0.0733 - val_kl_loss: 18.0852\n",
      "Epoch 51/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 728.9187 - reconstruction_loss: 0.0729 - kl_loss: 21.0444 - val_total_loss: 732.7800 - val_reconstruction_loss: 0.0733 - val_kl_loss: 20.4362\n",
      "Epoch 52/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 756.8270 - reconstruction_loss: 0.0757 - kl_loss: 21.2532 - val_total_loss: 747.2626 - val_reconstruction_loss: 0.0747 - val_kl_loss: 20.6845\n",
      "Epoch 53/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 737.9859 - reconstruction_loss: 0.0738 - kl_loss: 21.9122 - val_total_loss: 757.6302 - val_reconstruction_loss: 0.0757 - val_kl_loss: 19.3611\n",
      "Epoch 54/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 743.1815 - reconstruction_loss: 0.0743 - kl_loss: 21.6880 - val_total_loss: 819.7581 - val_reconstruction_loss: 0.0820 - val_kl_loss: 16.2146\n",
      "Epoch 55/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 756.9232 - reconstruction_loss: 0.0757 - kl_loss: 20.9827 - val_total_loss: 765.0180 - val_reconstruction_loss: 0.0765 - val_kl_loss: 20.5504\n",
      "Epoch 56/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 789.3812 - reconstruction_loss: 0.0789 - kl_loss: 21.7994 - val_total_loss: 758.3140 - val_reconstruction_loss: 0.0758 - val_kl_loss: 23.0329\n",
      "Epoch 57/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 736.2388 - reconstruction_loss: 0.0736 - kl_loss: 24.3004 - val_total_loss: 724.2185 - val_reconstruction_loss: 0.0724 - val_kl_loss: 24.7166\n",
      "Epoch 58/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 715.7176 - reconstruction_loss: 0.0715 - kl_loss: 26.2436 - val_total_loss: 705.6580 - val_reconstruction_loss: 0.0705 - val_kl_loss: 25.9607\n",
      "Epoch 59/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 699.3726 - reconstruction_loss: 0.0699 - kl_loss: 28.2438 - val_total_loss: 695.5018 - val_reconstruction_loss: 0.0695 - val_kl_loss: 28.2713\n",
      "Epoch 60/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 693.9948 - reconstruction_loss: 0.0694 - kl_loss: 30.0150 - val_total_loss: 694.1851 - val_reconstruction_loss: 0.0694 - val_kl_loss: 28.0051\n",
      "Epoch 61/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 691.9680 - reconstruction_loss: 0.0692 - kl_loss: 30.0605 - val_total_loss: 687.0283 - val_reconstruction_loss: 0.0687 - val_kl_loss: 28.6915\n",
      "Epoch 62/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 685.3032 - reconstruction_loss: 0.0685 - kl_loss: 30.4902 - val_total_loss: 679.5463 - val_reconstruction_loss: 0.0679 - val_kl_loss: 30.5172\n",
      "Epoch 63/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 681.2020 - reconstruction_loss: 0.0681 - kl_loss: 31.1366 - val_total_loss: 679.1861 - val_reconstruction_loss: 0.0679 - val_kl_loss: 29.9675\n",
      "Epoch 64/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 681.4140 - reconstruction_loss: 0.0681 - kl_loss: 32.1480 - val_total_loss: 697.6599 - val_reconstruction_loss: 0.0697 - val_kl_loss: 30.4272\n",
      "Epoch 65/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 707.3998 - reconstruction_loss: 0.0707 - kl_loss: 31.4001 - val_total_loss: 686.5800 - val_reconstruction_loss: 0.0686 - val_kl_loss: 29.8464\n",
      "Epoch 66/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 689.2525 - reconstruction_loss: 0.0689 - kl_loss: 31.1192 - val_total_loss: 684.4259 - val_reconstruction_loss: 0.0684 - val_kl_loss: 30.0591\n",
      "Epoch 67/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 683.6475 - reconstruction_loss: 0.0683 - kl_loss: 31.5785 - val_total_loss: 677.7595 - val_reconstruction_loss: 0.0677 - val_kl_loss: 31.2296\n",
      "Epoch 68/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 669.9327 - reconstruction_loss: 0.0670 - kl_loss: 33.2540 - val_total_loss: 667.8160 - val_reconstruction_loss: 0.0667 - val_kl_loss: 32.8768\n",
      "Epoch 69/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 785.7618 - reconstruction_loss: 0.0785 - kl_loss: 31.0630 - val_total_loss: 797.6387 - val_reconstruction_loss: 0.0797 - val_kl_loss: 27.3440\n",
      "Epoch 70/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 762.8680 - reconstruction_loss: 0.0763 - kl_loss: 27.4197 - val_total_loss: 732.4714 - val_reconstruction_loss: 0.0732 - val_kl_loss: 26.4529\n",
      "Epoch 71/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 736.0065 - reconstruction_loss: 0.0736 - kl_loss: 28.7993 - val_total_loss: 756.3249 - val_reconstruction_loss: 0.0756 - val_kl_loss: 25.3601\n",
      "Epoch 72/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 736.9388 - reconstruction_loss: 0.0737 - kl_loss: 26.5287 - val_total_loss: 711.8749 - val_reconstruction_loss: 0.0712 - val_kl_loss: 25.9188\n",
      "Epoch 73/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 724.3570 - reconstruction_loss: 0.0724 - kl_loss: 26.7831 - val_total_loss: 725.3355 - val_reconstruction_loss: 0.0725 - val_kl_loss: 26.2458\n",
      "Epoch 74/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 831.2193 - reconstruction_loss: 0.0831 - kl_loss: 21.1088 - val_total_loss: 787.3334 - val_reconstruction_loss: 0.0787 - val_kl_loss: 18.3351\n",
      "Epoch 75/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 787.4444 - reconstruction_loss: 0.0787 - kl_loss: 18.4574 - val_total_loss: 731.1020 - val_reconstruction_loss: 0.0731 - val_kl_loss: 19.6775\n",
      "Epoch 76/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 719.4629 - reconstruction_loss: 0.0719 - kl_loss: 21.6202 - val_total_loss: 705.7217 - val_reconstruction_loss: 0.0705 - val_kl_loss: 22.2585\n",
      "Epoch 77/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 713.5597 - reconstruction_loss: 0.0713 - kl_loss: 23.1552 - val_total_loss: 718.1537 - val_reconstruction_loss: 0.0718 - val_kl_loss: 21.2588\n",
      "Epoch 78/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 818.8286 - reconstruction_loss: 0.0819 - kl_loss: 15.9944 - val_total_loss: 756.3065 - val_reconstruction_loss: 0.0756 - val_kl_loss: 14.4409\n",
      "Epoch 79/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 754.0842 - reconstruction_loss: 0.0754 - kl_loss: 17.2505 - val_total_loss: 729.2960 - val_reconstruction_loss: 0.0729 - val_kl_loss: 18.0182\n",
      "Epoch 80/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 722.9185 - reconstruction_loss: 0.0723 - kl_loss: 20.0749 - val_total_loss: 720.1385 - val_reconstruction_loss: 0.0720 - val_kl_loss: 20.0973\n",
      "Epoch 81/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 713.3190 - reconstruction_loss: 0.0713 - kl_loss: 21.6532 - val_total_loss: 704.7963 - val_reconstruction_loss: 0.0705 - val_kl_loss: 21.4197\n",
      "Epoch 82/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 703.9860 - reconstruction_loss: 0.0704 - kl_loss: 23.2870 - val_total_loss: 701.2983 - val_reconstruction_loss: 0.0701 - val_kl_loss: 21.9071\n",
      "Epoch 83/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 708.2175 - reconstruction_loss: 0.0708 - kl_loss: 23.2531 - val_total_loss: 708.0869 - val_reconstruction_loss: 0.0708 - val_kl_loss: 22.6888\n",
      "Epoch 84/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 693.6633 - reconstruction_loss: 0.0693 - kl_loss: 24.2029 - val_total_loss: 686.8414 - val_reconstruction_loss: 0.0687 - val_kl_loss: 24.0016\n",
      "Epoch 85/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 717.7614 - reconstruction_loss: 0.0718 - kl_loss: 24.0218 - val_total_loss: 703.7895 - val_reconstruction_loss: 0.0704 - val_kl_loss: 23.5015\n",
      "Epoch 86/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 693.1815 - reconstruction_loss: 0.0693 - kl_loss: 25.6048 - val_total_loss: 684.3782 - val_reconstruction_loss: 0.0684 - val_kl_loss: 24.6899\n",
      "Epoch 87/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 720.3499 - reconstruction_loss: 0.0720 - kl_loss: 25.7828 - val_total_loss: 704.4384 - val_reconstruction_loss: 0.0704 - val_kl_loss: 26.1578\n",
      "Epoch 88/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 699.9415 - reconstruction_loss: 0.0700 - kl_loss: 26.6906 - val_total_loss: 688.1309 - val_reconstruction_loss: 0.0688 - val_kl_loss: 26.2669\n",
      "Epoch 89/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 681.2084 - reconstruction_loss: 0.0681 - kl_loss: 28.3479 - val_total_loss: 681.1624 - val_reconstruction_loss: 0.0681 - val_kl_loss: 26.2433\n",
      "Epoch 90/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 703.9351 - reconstruction_loss: 0.0704 - kl_loss: 27.3342 - val_total_loss: 676.5289 - val_reconstruction_loss: 0.0676 - val_kl_loss: 27.1692\n",
      "Epoch 91/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 701.6104 - reconstruction_loss: 0.0701 - kl_loss: 26.8489 - val_total_loss: 710.5214 - val_reconstruction_loss: 0.0710 - val_kl_loss: 24.6547\n",
      "Epoch 92/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 710.7986 - reconstruction_loss: 0.0711 - kl_loss: 26.3595 - val_total_loss: 706.8745 - val_reconstruction_loss: 0.0707 - val_kl_loss: 26.2646\n",
      "Epoch 93/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 680.5399 - reconstruction_loss: 0.0680 - kl_loss: 28.0710 - val_total_loss: 678.6411 - val_reconstruction_loss: 0.0678 - val_kl_loss: 27.1519\n",
      "Epoch 94/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 668.9510 - reconstruction_loss: 0.0669 - kl_loss: 29.4477 - val_total_loss: 681.5786 - val_reconstruction_loss: 0.0681 - val_kl_loss: 27.3860\n",
      "Epoch 95/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 678.3285 - reconstruction_loss: 0.0678 - kl_loss: 28.2709 - val_total_loss: 681.6246 - val_reconstruction_loss: 0.0681 - val_kl_loss: 28.3166\n",
      "Epoch 96/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 699.0004 - reconstruction_loss: 0.0699 - kl_loss: 28.0311 - val_total_loss: 680.3876 - val_reconstruction_loss: 0.0680 - val_kl_loss: 26.4095\n",
      "Epoch 97/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 673.5260 - reconstruction_loss: 0.0673 - kl_loss: 28.5172 - val_total_loss: 674.7744 - val_reconstruction_loss: 0.0674 - val_kl_loss: 27.9052\n",
      "Epoch 98/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 663.6550 - reconstruction_loss: 0.0663 - kl_loss: 29.3776 - val_total_loss: 663.5714 - val_reconstruction_loss: 0.0663 - val_kl_loss: 29.2817\n",
      "Epoch 99/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 655.3179 - reconstruction_loss: 0.0655 - kl_loss: 30.6843 - val_total_loss: 702.7635 - val_reconstruction_loss: 0.0702 - val_kl_loss: 28.7547\n",
      "Epoch 100/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 676.8917 - reconstruction_loss: 0.0677 - kl_loss: 31.0276 - val_total_loss: 716.5129 - val_reconstruction_loss: 0.0716 - val_kl_loss: 29.4797\n",
      "Epoch 101/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 673.3708 - reconstruction_loss: 0.0673 - kl_loss: 29.7621 - val_total_loss: 665.3691 - val_reconstruction_loss: 0.0665 - val_kl_loss: 29.7672\n",
      "Epoch 102/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 672.8791 - reconstruction_loss: 0.0673 - kl_loss: 30.5371 - val_total_loss: 676.2114 - val_reconstruction_loss: 0.0676 - val_kl_loss: 28.5936\n",
      "Epoch 103/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 665.7742 - reconstruction_loss: 0.0665 - kl_loss: 29.4859 - val_total_loss: 660.7174 - val_reconstruction_loss: 0.0660 - val_kl_loss: 28.8853\n",
      "Epoch 104/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 664.1041 - reconstruction_loss: 0.0664 - kl_loss: 28.8274 - val_total_loss: 683.2154 - val_reconstruction_loss: 0.0683 - val_kl_loss: 24.5994\n",
      "Epoch 105/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 659.5983 - reconstruction_loss: 0.0659 - kl_loss: 26.8359 - val_total_loss: 650.9197 - val_reconstruction_loss: 0.0651 - val_kl_loss: 27.0640\n",
      "Epoch 106/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 649.2001 - reconstruction_loss: 0.0649 - kl_loss: 28.9245 - val_total_loss: 650.3365 - val_reconstruction_loss: 0.0650 - val_kl_loss: 29.1593\n",
      "Epoch 107/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 674.7418 - reconstruction_loss: 0.0674 - kl_loss: 28.6799 - val_total_loss: 667.9646 - val_reconstruction_loss: 0.0668 - val_kl_loss: 27.7787\n",
      "Epoch 108/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 657.7377 - reconstruction_loss: 0.0657 - kl_loss: 27.6474 - val_total_loss: 651.6367 - val_reconstruction_loss: 0.0651 - val_kl_loss: 27.3839\n",
      "Epoch 109/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 642.6208 - reconstruction_loss: 0.0642 - kl_loss: 29.3298 - val_total_loss: 651.7734 - val_reconstruction_loss: 0.0651 - val_kl_loss: 29.6983\n",
      "Epoch 110/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 649.7027 - reconstruction_loss: 0.0649 - kl_loss: 32.4593 - val_total_loss: 656.9781 - val_reconstruction_loss: 0.0657 - val_kl_loss: 32.2060\n",
      "Epoch 111/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 634.1654 - reconstruction_loss: 0.0634 - kl_loss: 34.0748 - val_total_loss: 640.1557 - val_reconstruction_loss: 0.0640 - val_kl_loss: 32.8649\n",
      "Epoch 112/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 642.3351 - reconstruction_loss: 0.0642 - kl_loss: 33.1507 - val_total_loss: 664.0958 - val_reconstruction_loss: 0.0664 - val_kl_loss: 29.7536\n",
      "Epoch 113/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 733.7501 - reconstruction_loss: 0.0733 - kl_loss: 27.0820 - val_total_loss: 681.6036 - val_reconstruction_loss: 0.0681 - val_kl_loss: 26.9507\n",
      "Epoch 114/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 672.8010 - reconstruction_loss: 0.0673 - kl_loss: 29.2409 - val_total_loss: 681.0618 - val_reconstruction_loss: 0.0681 - val_kl_loss: 29.4721\n",
      "Epoch 115/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 662.2208 - reconstruction_loss: 0.0662 - kl_loss: 30.6549 - val_total_loss: 664.0308 - val_reconstruction_loss: 0.0664 - val_kl_loss: 30.1952\n",
      "Epoch 116/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 687.6323 - reconstruction_loss: 0.0687 - kl_loss: 29.6812 - val_total_loss: 679.0952 - val_reconstruction_loss: 0.0679 - val_kl_loss: 29.0021\n",
      "Epoch 117/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 673.1113 - reconstruction_loss: 0.0673 - kl_loss: 30.7707 - val_total_loss: 686.0690 - val_reconstruction_loss: 0.0686 - val_kl_loss: 31.2594\n",
      "Epoch 118/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 665.7875 - reconstruction_loss: 0.0665 - kl_loss: 30.1878 - val_total_loss: 669.8047 - val_reconstruction_loss: 0.0670 - val_kl_loss: 29.4554\n",
      "Epoch 119/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 678.5636 - reconstruction_loss: 0.0678 - kl_loss: 29.0309 - val_total_loss: 672.7004 - val_reconstruction_loss: 0.0672 - val_kl_loss: 28.1481\n",
      "Epoch 120/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 660.2409 - reconstruction_loss: 0.0660 - kl_loss: 29.8113 - val_total_loss: 653.2256 - val_reconstruction_loss: 0.0653 - val_kl_loss: 29.6533\n",
      "Epoch 121/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 665.6818 - reconstruction_loss: 0.0665 - kl_loss: 30.3226 - val_total_loss: 667.3766 - val_reconstruction_loss: 0.0667 - val_kl_loss: 28.7833\n",
      "Epoch 122/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 704.4119 - reconstruction_loss: 0.0704 - kl_loss: 29.3263 - val_total_loss: 813.4716 - val_reconstruction_loss: 0.0813 - val_kl_loss: 21.7061\n",
      "Epoch 123/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 758.9832 - reconstruction_loss: 0.0759 - kl_loss: 26.0541 - val_total_loss: 721.6062 - val_reconstruction_loss: 0.0721 - val_kl_loss: 27.1985\n",
      "Epoch 124/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 700.1228 - reconstruction_loss: 0.0700 - kl_loss: 27.9133 - val_total_loss: 671.0409 - val_reconstruction_loss: 0.0671 - val_kl_loss: 27.4880\n",
      "Epoch 125/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 705.4323 - reconstruction_loss: 0.0705 - kl_loss: 32.7304 - val_total_loss: 675.3508 - val_reconstruction_loss: 0.0675 - val_kl_loss: 32.9164\n",
      "Epoch 126/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 661.2230 - reconstruction_loss: 0.0661 - kl_loss: 34.2407 - val_total_loss: 674.7581 - val_reconstruction_loss: 0.0674 - val_kl_loss: 31.8317\n",
      "Epoch 127/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 661.5724 - reconstruction_loss: 0.0661 - kl_loss: 34.1731 - val_total_loss: 658.2289 - val_reconstruction_loss: 0.0658 - val_kl_loss: 33.5594\n",
      "Epoch 128/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 651.0165 - reconstruction_loss: 0.0651 - kl_loss: 36.0621 - val_total_loss: 663.6856 - val_reconstruction_loss: 0.0663 - val_kl_loss: 34.5152\n",
      "Epoch 129/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 648.5571 - reconstruction_loss: 0.0648 - kl_loss: 37.1791 - val_total_loss: 661.0180 - val_reconstruction_loss: 0.0661 - val_kl_loss: 35.7066\n",
      "Epoch 130/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 650.0021 - reconstruction_loss: 0.0650 - kl_loss: 36.9126 - val_total_loss: 674.6205 - val_reconstruction_loss: 0.0674 - val_kl_loss: 35.4381\n",
      "Epoch 131/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 690.0501 - reconstruction_loss: 0.0690 - kl_loss: 36.5740 - val_total_loss: 666.9036 - val_reconstruction_loss: 0.0667 - val_kl_loss: 34.5849\n",
      "Epoch 132/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 665.4144 - reconstruction_loss: 0.0665 - kl_loss: 35.1500 - val_total_loss: 650.1550 - val_reconstruction_loss: 0.0650 - val_kl_loss: 33.5962\n",
      "Epoch 133/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 654.4097 - reconstruction_loss: 0.0654 - kl_loss: 34.5807 - val_total_loss: 658.8082 - val_reconstruction_loss: 0.0658 - val_kl_loss: 33.9403\n",
      "Epoch 134/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 637.8305 - reconstruction_loss: 0.0637 - kl_loss: 35.7264 - val_total_loss: 648.2747 - val_reconstruction_loss: 0.0648 - val_kl_loss: 34.0364\n",
      "Epoch 135/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 642.5118 - reconstruction_loss: 0.0642 - kl_loss: 35.3777 - val_total_loss: 643.8251 - val_reconstruction_loss: 0.0643 - val_kl_loss: 34.0772\n",
      "Epoch 136/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 651.8919 - reconstruction_loss: 0.0652 - kl_loss: 34.7291 - val_total_loss: 731.9859 - val_reconstruction_loss: 0.0732 - val_kl_loss: 33.2047\n",
      "Epoch 137/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 718.4815 - reconstruction_loss: 0.0718 - kl_loss: 33.4741 - val_total_loss: 755.8314 - val_reconstruction_loss: 0.0756 - val_kl_loss: 27.7219\n",
      "Epoch 138/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 689.8743 - reconstruction_loss: 0.0690 - kl_loss: 30.5608 - val_total_loss: 681.7511 - val_reconstruction_loss: 0.0681 - val_kl_loss: 30.1751\n",
      "Epoch 139/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 662.9009 - reconstruction_loss: 0.0663 - kl_loss: 31.4526 - val_total_loss: 674.7252 - val_reconstruction_loss: 0.0674 - val_kl_loss: 30.0060\n",
      "Epoch 140/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 665.3494 - reconstruction_loss: 0.0665 - kl_loss: 31.0666 - val_total_loss: 668.1708 - val_reconstruction_loss: 0.0668 - val_kl_loss: 29.8843\n",
      "Epoch 141/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 646.1448 - reconstruction_loss: 0.0646 - kl_loss: 31.7505 - val_total_loss: 667.1142 - val_reconstruction_loss: 0.0667 - val_kl_loss: 30.2638\n",
      "Epoch 142/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 651.6815 - reconstruction_loss: 0.0651 - kl_loss: 32.6353 - val_total_loss: 662.7123 - val_reconstruction_loss: 0.0662 - val_kl_loss: 31.5701\n",
      "Epoch 143/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 641.7602 - reconstruction_loss: 0.0641 - kl_loss: 33.4454 - val_total_loss: 655.4368 - val_reconstruction_loss: 0.0655 - val_kl_loss: 32.1585\n",
      "Epoch 144/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 712.3093 - reconstruction_loss: 0.0712 - kl_loss: 31.2186 - val_total_loss: 695.6039 - val_reconstruction_loss: 0.0695 - val_kl_loss: 30.2201\n",
      "Epoch 145/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 664.7834 - reconstruction_loss: 0.0664 - kl_loss: 32.6713 - val_total_loss: 666.7084 - val_reconstruction_loss: 0.0666 - val_kl_loss: 31.7294\n",
      "Epoch 146/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 658.6165 - reconstruction_loss: 0.0658 - kl_loss: 33.3598 - val_total_loss: 671.5181 - val_reconstruction_loss: 0.0671 - val_kl_loss: 30.3746\n",
      "Epoch 147/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 647.0781 - reconstruction_loss: 0.0647 - kl_loss: 33.6309 - val_total_loss: 661.3209 - val_reconstruction_loss: 0.0661 - val_kl_loss: 32.7922\n",
      "Epoch 148/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 636.9011 - reconstruction_loss: 0.0637 - kl_loss: 34.3983 - val_total_loss: 648.6182 - val_reconstruction_loss: 0.0648 - val_kl_loss: 31.7498\n",
      "Epoch 149/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 638.7477 - reconstruction_loss: 0.0638 - kl_loss: 34.1835 - val_total_loss: 671.8337 - val_reconstruction_loss: 0.0672 - val_kl_loss: 31.1739\n",
      "Epoch 150/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 633.6113 - reconstruction_loss: 0.0633 - kl_loss: 34.1150 - val_total_loss: 644.8375 - val_reconstruction_loss: 0.0645 - val_kl_loss: 32.7172\n",
      "Epoch 151/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 626.7527 - reconstruction_loss: 0.0626 - kl_loss: 35.2096 - val_total_loss: 634.5817 - val_reconstruction_loss: 0.0634 - val_kl_loss: 33.0698\n",
      "Epoch 152/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 636.0198 - reconstruction_loss: 0.0636 - kl_loss: 33.2900 - val_total_loss: 643.1833 - val_reconstruction_loss: 0.0643 - val_kl_loss: 31.7441\n",
      "Epoch 153/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 657.3400 - reconstruction_loss: 0.0657 - kl_loss: 33.2219 - val_total_loss: 673.3044 - val_reconstruction_loss: 0.0673 - val_kl_loss: 32.2447\n",
      "Epoch 154/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 637.7171 - reconstruction_loss: 0.0637 - kl_loss: 34.8466 - val_total_loss: 646.7701 - val_reconstruction_loss: 0.0646 - val_kl_loss: 31.9584\n",
      "Epoch 155/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 748.9009 - reconstruction_loss: 0.0749 - kl_loss: 30.7242 - val_total_loss: 739.3155 - val_reconstruction_loss: 0.0739 - val_kl_loss: 27.1211\n",
      "Epoch 156/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 710.9005 - reconstruction_loss: 0.0711 - kl_loss: 29.4088 - val_total_loss: 697.3538 - val_reconstruction_loss: 0.0697 - val_kl_loss: 29.1919\n",
      "Epoch 157/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 675.6884 - reconstruction_loss: 0.0675 - kl_loss: 30.4571 - val_total_loss: 660.8114 - val_reconstruction_loss: 0.0661 - val_kl_loss: 29.9200\n",
      "Epoch 158/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 774.1446 - reconstruction_loss: 0.0774 - kl_loss: 32.4954 - val_total_loss: 747.8951 - val_reconstruction_loss: 0.0748 - val_kl_loss: 32.2821\n",
      "Epoch 159/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 707.5460 - reconstruction_loss: 0.0707 - kl_loss: 32.4111 - val_total_loss: 685.7465 - val_reconstruction_loss: 0.0685 - val_kl_loss: 30.6149\n",
      "Epoch 160/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 695.1141 - reconstruction_loss: 0.0695 - kl_loss: 31.6309 - val_total_loss: 691.3097 - val_reconstruction_loss: 0.0691 - val_kl_loss: 31.0750\n",
      "Epoch 161/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 697.5853 - reconstruction_loss: 0.0697 - kl_loss: 30.8454 - val_total_loss: 698.4009 - val_reconstruction_loss: 0.0698 - val_kl_loss: 28.8899\n",
      "Epoch 162/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 674.4018 - reconstruction_loss: 0.0674 - kl_loss: 30.8325 - val_total_loss: 672.5062 - val_reconstruction_loss: 0.0672 - val_kl_loss: 29.9011\n",
      "Epoch 163/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 665.8079 - reconstruction_loss: 0.0665 - kl_loss: 31.8767 - val_total_loss: 678.3218 - val_reconstruction_loss: 0.0678 - val_kl_loss: 30.4611\n",
      "Epoch 164/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 708.7213 - reconstruction_loss: 0.0708 - kl_loss: 28.8899 - val_total_loss: 716.0355 - val_reconstruction_loss: 0.0716 - val_kl_loss: 24.8791\n",
      "Epoch 165/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 693.7106 - reconstruction_loss: 0.0693 - kl_loss: 28.8659 - val_total_loss: 676.1976 - val_reconstruction_loss: 0.0676 - val_kl_loss: 28.7125\n",
      "Epoch 166/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 660.3165 - reconstruction_loss: 0.0660 - kl_loss: 33.5347 - val_total_loss: 658.8363 - val_reconstruction_loss: 0.0659 - val_kl_loss: 33.5094\n",
      "Epoch 167/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 643.7415 - reconstruction_loss: 0.0643 - kl_loss: 35.4208 - val_total_loss: 650.6879 - val_reconstruction_loss: 0.0650 - val_kl_loss: 34.2660\n",
      "Epoch 168/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 636.3304 - reconstruction_loss: 0.0636 - kl_loss: 36.3700 - val_total_loss: 648.7379 - val_reconstruction_loss: 0.0648 - val_kl_loss: 34.4583\n",
      "Epoch 169/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 637.9652 - reconstruction_loss: 0.0638 - kl_loss: 36.4945 - val_total_loss: 643.8969 - val_reconstruction_loss: 0.0644 - val_kl_loss: 34.8832\n",
      "Epoch 170/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 725.6906 - reconstruction_loss: 0.0725 - kl_loss: 31.8316 - val_total_loss: 702.6328 - val_reconstruction_loss: 0.0702 - val_kl_loss: 28.8125\n",
      "Epoch 171/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 672.2316 - reconstruction_loss: 0.0672 - kl_loss: 31.1384 - val_total_loss: 669.7973 - val_reconstruction_loss: 0.0669 - val_kl_loss: 30.3745\n",
      "Epoch 172/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 670.1826 - reconstruction_loss: 0.0670 - kl_loss: 31.5756 - val_total_loss: 665.0691 - val_reconstruction_loss: 0.0665 - val_kl_loss: 29.7938\n",
      "Epoch 173/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 648.1588 - reconstruction_loss: 0.0648 - kl_loss: 32.3991 - val_total_loss: 651.3730 - val_reconstruction_loss: 0.0651 - val_kl_loss: 30.8022\n",
      "Epoch 174/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 666.3745 - reconstruction_loss: 0.0666 - kl_loss: 31.6741 - val_total_loss: 653.4008 - val_reconstruction_loss: 0.0653 - val_kl_loss: 30.9700\n",
      "Epoch 175/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 646.2717 - reconstruction_loss: 0.0646 - kl_loss: 33.9712 - val_total_loss: 648.0649 - val_reconstruction_loss: 0.0648 - val_kl_loss: 32.6273\n",
      "Epoch 176/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 637.7007 - reconstruction_loss: 0.0637 - kl_loss: 34.7452 - val_total_loss: 644.1041 - val_reconstruction_loss: 0.0644 - val_kl_loss: 33.9736\n",
      "Epoch 177/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 629.5521 - reconstruction_loss: 0.0629 - kl_loss: 35.9867 - val_total_loss: 644.7376 - val_reconstruction_loss: 0.0644 - val_kl_loss: 34.7949\n",
      "Epoch 178/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 621.7507 - reconstruction_loss: 0.0621 - kl_loss: 37.0793 - val_total_loss: 632.5917 - val_reconstruction_loss: 0.0632 - val_kl_loss: 35.8204\n",
      "Epoch 179/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 627.7380 - reconstruction_loss: 0.0627 - kl_loss: 37.3971 - val_total_loss: 636.7080 - val_reconstruction_loss: 0.0636 - val_kl_loss: 35.6173\n",
      "Epoch 180/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 630.6888 - reconstruction_loss: 0.0630 - kl_loss: 35.8488 - val_total_loss: 645.7874 - val_reconstruction_loss: 0.0645 - val_kl_loss: 33.8045\n",
      "Epoch 181/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 626.1739 - reconstruction_loss: 0.0626 - kl_loss: 36.1368 - val_total_loss: 632.5461 - val_reconstruction_loss: 0.0632 - val_kl_loss: 36.0560\n",
      "Epoch 182/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 630.6416 - reconstruction_loss: 0.0630 - kl_loss: 37.0926 - val_total_loss: 653.1288 - val_reconstruction_loss: 0.0653 - val_kl_loss: 35.8590\n",
      "Epoch 183/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 626.4452 - reconstruction_loss: 0.0626 - kl_loss: 37.7404 - val_total_loss: 652.9479 - val_reconstruction_loss: 0.0653 - val_kl_loss: 36.0430\n",
      "Epoch 184/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 622.5756 - reconstruction_loss: 0.0622 - kl_loss: 38.2170 - val_total_loss: 634.0479 - val_reconstruction_loss: 0.0634 - val_kl_loss: 36.9524\n",
      "Epoch 185/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 616.8248 - reconstruction_loss: 0.0616 - kl_loss: 39.1820 - val_total_loss: 678.5179 - val_reconstruction_loss: 0.0678 - val_kl_loss: 36.2128\n",
      "Epoch 186/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 664.2731 - reconstruction_loss: 0.0664 - kl_loss: 38.3171 - val_total_loss: 673.0790 - val_reconstruction_loss: 0.0673 - val_kl_loss: 35.8376\n",
      "Epoch 187/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 635.7104 - reconstruction_loss: 0.0635 - kl_loss: 37.8101 - val_total_loss: 635.1483 - val_reconstruction_loss: 0.0635 - val_kl_loss: 37.1673\n",
      "Epoch 188/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 632.7402 - reconstruction_loss: 0.0632 - kl_loss: 39.5499 - val_total_loss: 1041.6343 - val_reconstruction_loss: 0.1041 - val_kl_loss: 40.4611\n",
      "Epoch 189/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 724.8911 - reconstruction_loss: 0.0725 - kl_loss: 35.9538 - val_total_loss: 641.1514 - val_reconstruction_loss: 0.0641 - val_kl_loss: 35.0184\n",
      "Epoch 190/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 629.0478 - reconstruction_loss: 0.0629 - kl_loss: 36.6738 - val_total_loss: 670.2914 - val_reconstruction_loss: 0.0670 - val_kl_loss: 33.3196\n",
      "Epoch 191/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 632.7289 - reconstruction_loss: 0.0632 - kl_loss: 36.9543 - val_total_loss: 648.8030 - val_reconstruction_loss: 0.0648 - val_kl_loss: 35.8365\n",
      "Epoch 192/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 632.9211 - reconstruction_loss: 0.0633 - kl_loss: 37.6036 - val_total_loss: 638.5732 - val_reconstruction_loss: 0.0638 - val_kl_loss: 37.4681\n",
      "Epoch 193/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 612.9512 - reconstruction_loss: 0.0613 - kl_loss: 39.2765 - val_total_loss: 628.5894 - val_reconstruction_loss: 0.0628 - val_kl_loss: 38.3387\n",
      "Epoch 194/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 608.0317 - reconstruction_loss: 0.0608 - kl_loss: 40.7700 - val_total_loss: 631.2534 - val_reconstruction_loss: 0.0631 - val_kl_loss: 39.2455\n",
      "Epoch 195/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 611.3990 - reconstruction_loss: 0.0611 - kl_loss: 41.2240 - val_total_loss: 630.1182 - val_reconstruction_loss: 0.0630 - val_kl_loss: 40.0661\n",
      "Epoch 196/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 611.7346 - reconstruction_loss: 0.0611 - kl_loss: 42.7977 - val_total_loss: 634.1010 - val_reconstruction_loss: 0.0634 - val_kl_loss: 41.0045\n",
      "Epoch 197/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 614.0656 - reconstruction_loss: 0.0614 - kl_loss: 41.8550 - val_total_loss: 623.4708 - val_reconstruction_loss: 0.0623 - val_kl_loss: 39.3902\n",
      "Epoch 198/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 602.6899 - reconstruction_loss: 0.0602 - kl_loss: 42.8263 - val_total_loss: 621.6523 - val_reconstruction_loss: 0.0621 - val_kl_loss: 41.1442\n",
      "Epoch 199/200\n",
      "109/109 [==============================] - 1s 9ms/step - total_loss: 594.9460 - reconstruction_loss: 0.0595 - kl_loss: 43.3660 - val_total_loss: 622.4080 - val_reconstruction_loss: 0.0622 - val_kl_loss: 41.4144\n",
      "Epoch 200/200\n",
      "109/109 [==============================] - 1s 10ms/step - total_loss: 618.9606 - reconstruction_loss: 0.0619 - kl_loss: 44.5268 - val_total_loss: 623.2048 - val_reconstruction_loss: 0.0623 - val_kl_loss: 41.9979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fe65d210>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "vae.fit(data, epochs = EPOCHS, batch_size = BATCH_SIZE, shuffle = True, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_board(tensor):\n",
    "    board = [['.' for j in range(8)] for i in range(8)]\n",
    "    for i, p in enumerate('KQRBNPkqrbnp'):\n",
    "        for r in range(8):\n",
    "            for c in range(8):\n",
    "                if tensor[r][c][i] > 0.4:\n",
    "                    if board[r][c] != '.':\n",
    "                        print(f'conflict between {board[r][c]} and {p} at position ({r},{c})')\n",
    "                        continue\n",
    "                    board[r][c] = p\n",
    "    pprint.pprint(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['r', '.', '.', '.', '.', '.', '.', '.'],\n",
      " ['p', 'p', '.', '.', '.', '.', '.', '.'],\n",
      " ['.', '.', '.', '.', '.', '.', '.', '.'],\n",
      " ['.', '.', '.', '.', '.', '.', '.', '.'],\n",
      " ['.', '.', 'P', 'P', '.', '.', '.', '.'],\n",
      " ['.', '.', 'N', '.', '.', '.', '.', '.'],\n",
      " ['P', 'P', '.', '.', '.', 'P', 'P', 'P'],\n",
      " ['R', 'R', '.', 'Q', 'K', 'B', '.', 'R']]\n",
      "\n",
      "[['r', '.', '.', '.', 'k', 'b', '.', 'r'],\n",
      " ['p', 'p', '.', '.', '.', 'p', '.', '.'],\n",
      " ['.', '.', '.', '.', '.', '.', '.', '.'],\n",
      " ['.', '.', '.', '.', '.', '.', '.', '.'],\n",
      " ['.', '.', '.', '.', '.', '.', '.', '.'],\n",
      " ['.', '.', '.', '.', '.', '.', '.', '.'],\n",
      " ['P', 'P', 'P', '.', '.', '.', '.', 'P'],\n",
      " ['R', 'N', '.', 'R', '.', '.', '.', 'R']]\n",
      "\n",
      "[['r', 'n', 'b', 'q', '.', 'r', 'k', '.'],\n",
      " ['p', 'p', 'p', 'p', '.', 'p', 'p', '.'],\n",
      " ['.', '.', '.', '.', '.', '.', '.', '.'],\n",
      " ['.', '.', '.', '.', '.', '.', '.', '.'],\n",
      " ['.', '.', 'P', 'P', 'P', '.', '.', '.'],\n",
      " ['.', '.', 'N', '.', '.', '.', '.', '.'],\n",
      " ['P', 'P', '.', '.', '.', 'P', 'P', 'P'],\n",
      " ['R', '.', 'B', 'Q', 'K', 'B', 'N', 'R']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for reconstruction in vae.generate_sample(3):\n",
    "    print_board(reconstruction)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
