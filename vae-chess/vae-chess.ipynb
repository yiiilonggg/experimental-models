{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational AutoEncoder Chess Position Generator\n",
    "\n",
    "##### Inspiration\n",
    "* Recently, I have been reading up about generative models, and one of them that caught my eye was the VAE.\n",
    "* It allows you to generate new data that is similar to your training data.\n",
    "* At the same time, I am interested in chess and have enjoyed solving chess puzzles for quite awhile.\n",
    "* However, the premise of a chess puzzle is that the player knows that there exists a optimal move / sequence of moves that provides the player an advantage.\n",
    "* This helps the player to improve in terms of tactics and pattern recognition, but in most cases when playing a game of chess, we do not know if there exists an optimal solution.\n",
    "* This introduces the idea of an anti-puzzle, where the premise is now that the chess position provided may have an optimal solution, or the \"solution\" is to play a move that maintains the status-quo.\n",
    "* With the VAE, we can train it with a training set of legal chess positions, and have it output more chess positions.\n",
    "* Since the VAE would not have any idea if the chess position has an optimal solution or not, it is perfect for creating \"anti-puzzle\" solutions.\n",
    "* Furthermore, chess is a \"constrained\" game, where the rules are clear and we can check if the position generated by the VAE is a legal position or not.\n",
    "* For this model, the goal is to simply generate new (legal) chess positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from scipy.stats import norm\n",
    "from keras import layers, models, metrics, losses, optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Collection\n",
    "\n",
    "* The easiest way to obtain chess is positions is from my own games.\n",
    "* I exported move data from some chess games that I have played online in Lichess, which comes in a .pgn file.\n",
    "* From this file, we can get the move orders for the games that I have exported, from which I can deduce the chess positions.\n",
    "* For this, I used the python-chess library, which helps to deduce FEN positions from PGN move list\n",
    "* Once we get the FEN positions, we can derive the values for the input data we wish to parse into our model\n",
    "\n",
    "##### Data Representation\n",
    "* Although this doesn't give the chess positions directly, we can manipulate it into a form that works for the VAE.\n",
    "* The current idea is to have a 8 x 8 x 12 matrix, which means to say each of the 12 pieces (K, Q, R, B, N, P, k, q, r, b, n, p) each have their own 8 x 8 chessboard that denotes their position.\n",
    "* We can generate these as all chess games I exported start from the standard position, and we can denote the piece at a certain position with a 1 (i.e. 0 marks that the piece is not at that position).\n",
    "* This coincidentally is a perfect data set for generating anti-puzzles as it is formed from the sequence of moves of a game, of which not all positions have an optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = os.path.dirname(__vsc_ipynb_file__)\n",
    "fen_data_path = os.path.join(DIR, \"data\", \"fen-data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIECE_TO_IDX = dict([[c, i] for i, c in enumerate('KQRBNPkqrbnp')])\n",
    "\n",
    "def generate_matrix_from_fen(fen_string):\n",
    "    # initialise board\n",
    "    board = [[[0 for k in range(12)] for j in range(8)] for i in range(8)]\n",
    "\n",
    "    # process FEN string\n",
    "    board_string = fen_string.split(\" \")[0].split(\"/\")\n",
    "    row, col = 0, 0\n",
    "    for board_row in board_string:\n",
    "        for row_item in board_row:\n",
    "            if row_item.isnumeric():\n",
    "                col += int(row_item)\n",
    "            else:\n",
    "                board[row][col][PIECE_TO_IDX[row_item]] = 1\n",
    "                col += 1\n",
    "        row += 1\n",
    "        col = 0\n",
    "    return board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(fen_data_path) as file:\n",
    "    for line in file:\n",
    "        data.append(np.array(generate_matrix_from_fen(line)))\n",
    "data = np.array(data, 'float64')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (hyper)parameters\n",
    "latent_dims = 8\n",
    "hidden_layers = 3\n",
    "base_units = 2 << 5\n",
    "kernel_size = (3, 3)\n",
    "strides = 2\n",
    "dropout_rate = 0.3\n",
    "beta = 10 ** -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape = (batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class vae_chess(models.Model):\n",
    "\n",
    "    def __init__(self, latent_dims, hidden_layers, base_units, kernel_size, strides, dropout_rate, beta):\n",
    "        super(vae_chess, self).__init__()\n",
    "\n",
    "        self.latent_dims = latent_dims\n",
    "        self.beta = beta\n",
    "\n",
    "        self.encoder = self.generate_encoder_model(hidden_layers, base_units, kernel_size, strides, dropout_rate)\n",
    "        self.decoder = self.generate_decoder_model(hidden_layers, base_units, kernel_size, strides, dropout_rate)\n",
    "        print(self.encoder.summary())\n",
    "        print(self.decoder.summary())\n",
    "\n",
    "        self.total_loss_tracker = metrics.Mean(name = \"total_loss\")\n",
    "        self.reconstruction_loss_tracker = metrics.Mean(name = \"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = metrics.Mean(name = \"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.total_loss_tracker, self.reconstruction_loss_tracker, self.kl_loss_tracker]\n",
    "\n",
    "    def generate_encoder_model(self, hidden_layers, base_units, kernel_size, strides, dropout_rate):\n",
    "        encoder_input = layers.Input(shape = (8, 8, 12), name = \"encoder_input\")\n",
    "\n",
    "        for i in range(hidden_layers):\n",
    "            conv_layer = layers.Conv2D(base_units << i, kernel_size, strides, padding = \"same\")(encoder_input if i == 0 else dropout_layer)\n",
    "            batch_norm_layer = layers.BatchNormalization()(conv_layer)\n",
    "            activation_layer = layers.Activation('relu')(batch_norm_layer)\n",
    "            dropout_layer = layers.Dropout(dropout_rate)(activation_layer)\n",
    "        self.pass_back_shape = K.int_shape(dropout_layer)[1:]\n",
    "\n",
    "        flatten_layer = layers.Flatten()(dropout_layer)\n",
    "        z_mean = layers.Dense(self.latent_dims, name = \"z_mean\")(flatten_layer)\n",
    "        z_log_var = layers.Dense(self.latent_dims, name = \"z_log_var\")(flatten_layer)\n",
    "        z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "        return models.Model(encoder_input, [z_mean, z_log_var, z], name = \"encoder\")\n",
    "    \n",
    "    def generate_decoder_model(self, hidden_layers, base_units, kernel_size, strides, dropout_rate):\n",
    "        decoder_input = layers.Input(shape = (self.latent_dims), name = \"decoder_input\")\n",
    "\n",
    "        before_reshape = layers.Dense(np.prod(self.pass_back_shape))(decoder_input)\n",
    "        reshape_layer = layers.Reshape(self.pass_back_shape)(before_reshape)\n",
    "\n",
    "        for i in range(hidden_layers - 1, -1, -1):\n",
    "            conv_transpose_layer = layers.Conv2DTranspose(base_units << i, kernel_size, strides, padding = \"same\")(reshape_layer if i == hidden_layers - 1 else dropout_layer)\n",
    "            batch_norm_layer = layers.BatchNormalization()(conv_transpose_layer)\n",
    "            activation_layer = layers.Activation('relu')(batch_norm_layer)\n",
    "            dropout_layer = layers.Dropout(dropout_rate)(activation_layer)\n",
    "\n",
    "        decoder_output = layers.Conv2DTranspose(12, kernel_size, 1, padding = \"same\")(dropout_layer)\n",
    "\n",
    "        return models.Model(decoder_input, decoder_output, name = \"decoder\")\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return z_mean, z_log_var, reconstruction\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, reconstruction = self(data)\n",
    "            reconstruction_loss = tf.reduce_mean(losses.binary_crossentropy(data, reconstruction, axis = (1, 2, 3)))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(-0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)), axis = 1))\n",
    "            total_loss = reconstruction_loss + self.beta * kl_loss\n",
    "        \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {m.name : m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        z_mean, z_log_var, reconstruction = self(data)\n",
    "        reconstruction_loss = tf.reduce_mean(losses.binary_crossentropy(data, reconstruction, axis = (1, 2, 3)))\n",
    "        kl_loss = tf.reduce_mean(tf.reduce_sum(-0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)), axis = 1))\n",
    "        total_loss = reconstruction_loss + self.beta * kl_loss\n",
    "        \n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {m.name : m.result() for m in self.metrics}\n",
    "\n",
    "vae = vae_chess(latent_dims, hidden_layers, base_units, kernel_size, strides, dropout_rate, beta)\n",
    "vae.compile(optimizer = \"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "vae.fit(data, epochs = EPOCHS, batch_size = BATCH_SIZE, shuffle = True, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
