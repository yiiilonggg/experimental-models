{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational AutoEncoder Chess Position Generator\n",
    "\n",
    "##### Inspiration\n",
    "* Recently, I have been reading up about generative models, and one of them that caught my eye was the VAE.\n",
    "* It allows you to generate new data that is similar to your training data.\n",
    "* At the same time, I am interested in chess and have enjoyed solving chess puzzles for quite awhile.\n",
    "* However, the premise of a chess puzzle is that the player knows that there exists a optimal move / sequence of moves that provides the player an advantage.\n",
    "* This helps the player to improve in terms of tactics and pattern recognition, but in most cases when playing a game of chess, we do not know if there exists an optimal solution.\n",
    "* This introduces the idea of an anti-puzzle, where the premise is now that the chess position provided may have an optimal solution, or the \"solution\" is to play a move that maintains the status-quo.\n",
    "* With the VAE, we can train it with a training set of legal chess positions, and have it output more chess positions.\n",
    "* Since the VAE would not have any idea if the chess position has an optimal solution or not, it is perfect for creating \"anti-puzzle\" solutions.\n",
    "* Furthermore, chess is a \"constrained\" game, where the rules are clear and we can check if the position generated by the VAE is a legal position or not.\n",
    "* For this model, the goal is to simply generate new (legal) chess positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pprint\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from scipy.stats import norm\n",
    "from keras import layers, models, metrics, losses, optimizers, activations\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Collection\n",
    "\n",
    "* The easiest way to obtain chess is positions is from my own games.\n",
    "* I exported move data from some chess games that I have played online in Lichess, which comes in a .pgn file.\n",
    "* From this file, we can get the move orders for the games that I have exported, from which I can deduce the chess positions.\n",
    "* For this, I used the python-chess library, which helps to deduce FEN positions from PGN move list\n",
    "* Once we get the FEN positions, we can derive the values for the input data we wish to parse into our model\n",
    "\n",
    "##### Data Representation\n",
    "* Although this doesn't give the chess positions directly, we can manipulate it into a form that works for the VAE.\n",
    "* The current idea is to have a 8 x 8 x 12 matrix, which means to say each of the 12 pieces (K, Q, R, B, N, P, k, q, r, b, n, p) each have their own 8 x 8 chessboard that denotes their position.\n",
    "* We can generate these as all chess games I exported start from the standard position, and we can denote the piece at a certain position with a 1 (i.e. 0 marks that the piece is not at that position).\n",
    "* This coincidentally is a perfect data set for generating anti-puzzles as it is formed from the sequence of moves of a game, of which not all positions have an optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = os.path.dirname(__vsc_ipynb_file__)\n",
    "fen_data_path = os.path.join(DIR, \"data\", \"fen-data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIECE_TO_IDX = dict([[c, i] for i, c in enumerate('KQRBNPkqrbnp')])\n",
    "\n",
    "def generate_matrix_from_fen(fen_string):\n",
    "    # initialise board\n",
    "    board = [[[0 for k in range(12)] for j in range(8)] for i in range(8)]\n",
    "\n",
    "    # process FEN string\n",
    "    board_string = fen_string.split(\" \")[0].split(\"/\")\n",
    "    row, col = 0, 0\n",
    "    for board_row in board_string:\n",
    "        for row_item in board_row:\n",
    "            if row_item.isnumeric():\n",
    "                col += int(row_item)\n",
    "            else:\n",
    "                board[row][col][PIECE_TO_IDX[row_item]] = 1\n",
    "                col += 1\n",
    "        row += 1\n",
    "        col = 0\n",
    "    return board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18175, 8, 8, 12)\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "with open(fen_data_path) as file:\n",
    "    for line in file:\n",
    "        data.append(np.array(generate_matrix_from_fen(line)))\n",
    "data = np.array(data, 'float64')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (hyper)parameters\n",
    "latent_dims = 8\n",
    "hidden_layers = 3\n",
    "base_units = 2 << 3\n",
    "kernel_size = (2, 2)\n",
    "strides = 2\n",
    "dropout_rate = 0.3\n",
    "threshold = 0.3\n",
    "beta_1 = 10 ** 4\n",
    "beta_2 = 10 ** -2\n",
    "learning_rate = 10 ** -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 8, 8, 12)]   0           []                               \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)             (None, 4, 4, 16)     784         ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_132 (Batch  (None, 4, 4, 16)    64          ['conv2d_66[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_132 (Activation)    (None, 4, 4, 16)     0           ['batch_normalization_132[0][0]']\n",
      "                                                                                                  \n",
      " dropout_132 (Dropout)          (None, 4, 4, 16)     0           ['activation_132[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)             (None, 2, 2, 32)     2080        ['dropout_132[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_133 (Batch  (None, 2, 2, 32)    128         ['conv2d_67[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_133 (Activation)    (None, 2, 2, 32)     0           ['batch_normalization_133[0][0]']\n",
      "                                                                                                  \n",
      " dropout_133 (Dropout)          (None, 2, 2, 32)     0           ['activation_133[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)             (None, 1, 1, 64)     8256        ['dropout_133[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_134 (Batch  (None, 1, 1, 64)    256         ['conv2d_68[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_134 (Activation)    (None, 1, 1, 64)     0           ['batch_normalization_134[0][0]']\n",
      "                                                                                                  \n",
      " dropout_134 (Dropout)          (None, 1, 1, 64)     0           ['activation_134[0][0]']         \n",
      "                                                                                                  \n",
      " flatten_22 (Flatten)           (None, 64)           0           ['dropout_134[0][0]']            \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 8)            520         ['flatten_22[0][0]']             \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 8)            520         ['flatten_22[0][0]']             \n",
      "                                                                                                  \n",
      " sampling_22 (Sampling)         (None, 8)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,608\n",
      "Trainable params: 12,384\n",
      "Non-trainable params: 224\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 64)                576       \n",
      "                                                                 \n",
      " reshape_22 (Reshape)        (None, 1, 1, 64)          0         \n",
      "                                                                 \n",
      " conv2d_transpose_88 (Conv2D  (None, 2, 2, 64)         16448     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_135 (Ba  (None, 2, 2, 64)         256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_135 (Activation)  (None, 2, 2, 64)         0         \n",
      "                                                                 \n",
      " dropout_135 (Dropout)       (None, 2, 2, 64)          0         \n",
      "                                                                 \n",
      " conv2d_transpose_89 (Conv2D  (None, 4, 4, 32)         8224      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_136 (Ba  (None, 4, 4, 32)         128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_136 (Activation)  (None, 4, 4, 32)         0         \n",
      "                                                                 \n",
      " dropout_136 (Dropout)       (None, 4, 4, 32)          0         \n",
      "                                                                 \n",
      " conv2d_transpose_90 (Conv2D  (None, 8, 8, 16)         2064      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_137 (Ba  (None, 8, 8, 16)         64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_137 (Activation)  (None, 8, 8, 16)         0         \n",
      "                                                                 \n",
      " dropout_137 (Dropout)       (None, 8, 8, 16)          0         \n",
      "                                                                 \n",
      " conv2d_transpose_91 (Conv2D  (None, 8, 8, 12)         780       \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " tf.math.tanh_2 (TFOpLambda)  (None, 8, 8, 12)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,540\n",
      "Trainable params: 28,316\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape = (batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class vae_chess(models.Model):\n",
    "\n",
    "    def __init__(self, latent_dims, hidden_layers, base_units, kernel_size, strides, dropout_rate, threshold, beta_1, beta_2):\n",
    "        super(vae_chess, self).__init__()\n",
    "\n",
    "        self.latent_dims = latent_dims\n",
    "        self.threshold = threshold\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "        self.encoder = self.generate_encoder_model(hidden_layers, base_units, kernel_size, strides, dropout_rate)\n",
    "        self.decoder = self.generate_decoder_model(hidden_layers, base_units, kernel_size, strides, dropout_rate)\n",
    "        print(self.encoder.summary())\n",
    "        print(self.decoder.summary())\n",
    "\n",
    "        self.total_loss_tracker = metrics.Mean(name = \"total_loss\")\n",
    "        self.reconstruction_loss_tracker = metrics.Mean(name = \"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = metrics.Mean(name = \"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.total_loss_tracker, self.reconstruction_loss_tracker, self.kl_loss_tracker]\n",
    "\n",
    "    def generate_encoder_model(self, hidden_layers, base_units, kernel_size, strides, dropout_rate):\n",
    "        encoder_input = layers.Input(shape = (8, 8, 12), name = \"encoder_input\")\n",
    "\n",
    "        for i in range(hidden_layers):\n",
    "            conv_layer = layers.Conv2D(base_units << i, kernel_size, strides, padding = \"same\")(encoder_input if i == 0 else dropout_layer)\n",
    "            batch_norm_layer = layers.BatchNormalization()(conv_layer)\n",
    "            activation_layer = layers.Activation('relu')(batch_norm_layer)\n",
    "            dropout_layer = layers.Dropout(dropout_rate)(activation_layer)\n",
    "        self.pass_back_shape = K.int_shape(dropout_layer)[1:]\n",
    "\n",
    "        flatten_layer = layers.Flatten()(dropout_layer)\n",
    "        z_mean = layers.Dense(self.latent_dims, name = \"z_mean\")(flatten_layer)\n",
    "        z_log_var = layers.Dense(self.latent_dims, name = \"z_log_var\")(flatten_layer)\n",
    "        z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "        return models.Model(encoder_input, [z_mean, z_log_var, z], name = \"encoder\")\n",
    "    \n",
    "    def generate_decoder_model(self, hidden_layers, base_units, kernel_size, strides, dropout_rate):\n",
    "        decoder_input = layers.Input(shape = (self.latent_dims), name = \"decoder_input\")\n",
    "\n",
    "        before_reshape = layers.Dense(np.prod(self.pass_back_shape))(decoder_input)\n",
    "        reshape_layer = layers.Reshape(self.pass_back_shape)(before_reshape)\n",
    "\n",
    "        for i in range(hidden_layers - 1, -1, -1):\n",
    "            conv_transpose_layer = layers.Conv2DTranspose(base_units << i, kernel_size, strides, padding = \"same\")(reshape_layer if i == hidden_layers - 1 else dropout_layer)\n",
    "            batch_norm_layer = layers.BatchNormalization()(conv_transpose_layer)\n",
    "            activation_layer = layers.Activation('relu')(batch_norm_layer)\n",
    "            dropout_layer = layers.Dropout(dropout_rate)(activation_layer)\n",
    "\n",
    "        decoder_output = layers.Conv2DTranspose(12, kernel_size, 1, padding = \"same\")(dropout_layer)\n",
    "        decoder_output_transformed = activations.tanh(decoder_output)\n",
    "\n",
    "        return models.Model(decoder_input, decoder_output_transformed, name = \"decoder\")\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstruction = self.decoder(z)\n",
    "        return z_mean, z_log_var, reconstruction\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, reconstruction = self(data)\n",
    "            reconstruction_loss = tf.reduce_mean(losses.binary_crossentropy(data, reconstruction, axis = (1, 2, 3)))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(-0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)), axis = 1))\n",
    "            total_loss = self.beta_1 + reconstruction_loss + self.beta_2 * kl_loss\n",
    "        \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {m.name : m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        z_mean, z_log_var, reconstruction = self(data)\n",
    "        reconstruction_loss = tf.reduce_mean(losses.binary_crossentropy(data, reconstruction, axis = (1, 2, 3)))\n",
    "        kl_loss = tf.reduce_mean(tf.reduce_sum(-0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)), axis = 1))\n",
    "        total_loss = self.beta_1 + reconstruction_loss + self.beta_2 * kl_loss\n",
    "        \n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {m.name : m.result() for m in self.metrics}\n",
    "    \n",
    "    def generate_sample(self, num_samples):\n",
    "        z_sample = tf.random.normal(shape = (num_samples, self.latent_dims))\n",
    "        return self.decoder(z_sample)\n",
    "\n",
    "vae = vae_chess(latent_dims, hidden_layers, base_units, kernel_size, strides, dropout_rate, threshold, beta_1, beta_2)\n",
    "optimiser = optimizers.Adam(learning_rate = learning_rate)\n",
    "vae.compile(optimizer = \"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "64/64 [==============================] - 3s 15ms/step - total_loss: 10000.1514 - reconstruction_loss: 0.1540 - kl_loss: 0.0327 - val_total_loss: 10000.1289 - val_reconstruction_loss: 0.1288 - val_kl_loss: 0.0247\n",
      "Epoch 2/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.1250 - reconstruction_loss: 0.1269 - kl_loss: 0.0268 - val_total_loss: 10000.1250 - val_reconstruction_loss: 0.1242 - val_kl_loss: 0.0214\n",
      "Epoch 3/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.1240 - reconstruction_loss: 0.1225 - kl_loss: 0.0148 - val_total_loss: 10000.1211 - val_reconstruction_loss: 0.1203 - val_kl_loss: 0.0103\n",
      "Epoch 4/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.1230 - reconstruction_loss: 0.1185 - kl_loss: 0.0132 - val_total_loss: 10000.1172 - val_reconstruction_loss: 0.1172 - val_kl_loss: 0.0183\n",
      "Epoch 5/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.1201 - reconstruction_loss: 0.1143 - kl_loss: 0.0227 - val_total_loss: 10000.1152 - val_reconstruction_loss: 0.1143 - val_kl_loss: 0.0276\n",
      "Epoch 6/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.1182 - reconstruction_loss: 0.1113 - kl_loss: 0.0369 - val_total_loss: 10000.1123 - val_reconstruction_loss: 0.1125 - val_kl_loss: 0.0356\n",
      "Epoch 7/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.1191 - reconstruction_loss: 0.1108 - kl_loss: 0.0391 - val_total_loss: 10000.1113 - val_reconstruction_loss: 0.1102 - val_kl_loss: 0.0401\n",
      "Epoch 8/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.1172 - reconstruction_loss: 0.1104 - kl_loss: 0.0456 - val_total_loss: 10000.1113 - val_reconstruction_loss: 0.1098 - val_kl_loss: 0.0444\n",
      "Epoch 9/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.1104 - reconstruction_loss: 0.1073 - kl_loss: 0.0546 - val_total_loss: 10000.1104 - val_reconstruction_loss: 0.1096 - val_kl_loss: 0.0650\n",
      "Epoch 10/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.1055 - reconstruction_loss: 0.1054 - kl_loss: 0.0636 - val_total_loss: 10000.1084 - val_reconstruction_loss: 0.1072 - val_kl_loss: 0.0586\n",
      "Epoch 11/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.1045 - reconstruction_loss: 0.1026 - kl_loss: 0.0647 - val_total_loss: 10000.1074 - val_reconstruction_loss: 0.1059 - val_kl_loss: 0.0703\n",
      "Epoch 12/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.1035 - reconstruction_loss: 0.1025 - kl_loss: 0.1274 - val_total_loss: 10000.1113 - val_reconstruction_loss: 0.1087 - val_kl_loss: 0.2919\n",
      "Epoch 13/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.1055 - reconstruction_loss: 0.1040 - kl_loss: 0.1664 - val_total_loss: 10000.1064 - val_reconstruction_loss: 0.1054 - val_kl_loss: 0.1349\n",
      "Epoch 14/200\n",
      "64/64 [==============================] - 1s 9ms/step - total_loss: 10000.1016 - reconstruction_loss: 0.1002 - kl_loss: 0.1328 - val_total_loss: 10000.1035 - val_reconstruction_loss: 0.1025 - val_kl_loss: 0.1326\n",
      "Epoch 15/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.1006 - reconstruction_loss: 0.0987 - kl_loss: 0.1210 - val_total_loss: 10000.1035 - val_reconstruction_loss: 0.1015 - val_kl_loss: 0.1197\n",
      "Epoch 16/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.1006 - reconstruction_loss: 0.0974 - kl_loss: 0.1291 - val_total_loss: 10000.1025 - val_reconstruction_loss: 0.1012 - val_kl_loss: 0.1714\n",
      "Epoch 17/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.1006 - reconstruction_loss: 0.0985 - kl_loss: 0.2231 - val_total_loss: 10000.1074 - val_reconstruction_loss: 0.1056 - val_kl_loss: 0.2597\n",
      "Epoch 18/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.1035 - reconstruction_loss: 0.1000 - kl_loss: 0.1725 - val_total_loss: 10000.1025 - val_reconstruction_loss: 0.1017 - val_kl_loss: 0.1345\n",
      "Epoch 19/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.1006 - reconstruction_loss: 0.0969 - kl_loss: 0.1477 - val_total_loss: 10000.1016 - val_reconstruction_loss: 0.1000 - val_kl_loss: 0.1688\n",
      "Epoch 20/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.1006 - reconstruction_loss: 0.0986 - kl_loss: 0.2782 - val_total_loss: 10000.1045 - val_reconstruction_loss: 0.1026 - val_kl_loss: 0.2435\n",
      "Epoch 21/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.1016 - reconstruction_loss: 0.0963 - kl_loss: 0.1972 - val_total_loss: 10000.1016 - val_reconstruction_loss: 0.0995 - val_kl_loss: 0.1847\n",
      "Epoch 22/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0996 - reconstruction_loss: 0.0949 - kl_loss: 0.1952 - val_total_loss: 10000.1016 - val_reconstruction_loss: 0.0993 - val_kl_loss: 0.1913\n",
      "Epoch 23/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0996 - reconstruction_loss: 0.0947 - kl_loss: 0.2401 - val_total_loss: 10000.1016 - val_reconstruction_loss: 0.0990 - val_kl_loss: 0.2008\n",
      "Epoch 24/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0957 - reconstruction_loss: 0.0935 - kl_loss: 0.2011 - val_total_loss: 10000.0996 - val_reconstruction_loss: 0.0967 - val_kl_loss: 0.2276\n",
      "Epoch 25/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0996 - reconstruction_loss: 0.0943 - kl_loss: 0.1672 - val_total_loss: 10000.0996 - val_reconstruction_loss: 0.0983 - val_kl_loss: 0.1572\n",
      "Epoch 26/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0996 - reconstruction_loss: 0.0936 - kl_loss: 0.1705 - val_total_loss: 10000.0977 - val_reconstruction_loss: 0.0973 - val_kl_loss: 0.1505\n",
      "Epoch 27/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0977 - reconstruction_loss: 0.0923 - kl_loss: 0.1846 - val_total_loss: 10000.1016 - val_reconstruction_loss: 0.0979 - val_kl_loss: 0.2873\n",
      "Epoch 28/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0928 - reconstruction_loss: 0.0929 - kl_loss: 0.2086 - val_total_loss: 10000.0977 - val_reconstruction_loss: 0.0957 - val_kl_loss: 0.2022\n",
      "Epoch 29/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0928 - reconstruction_loss: 0.0910 - kl_loss: 0.2409 - val_total_loss: 10000.0967 - val_reconstruction_loss: 0.0943 - val_kl_loss: 0.2539\n",
      "Epoch 30/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0928 - reconstruction_loss: 0.0908 - kl_loss: 0.2003 - val_total_loss: 10000.0967 - val_reconstruction_loss: 0.0948 - val_kl_loss: 0.2262\n",
      "Epoch 31/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0898 - reconstruction_loss: 0.0903 - kl_loss: 0.2082 - val_total_loss: 10000.0967 - val_reconstruction_loss: 0.0947 - val_kl_loss: 0.2152\n",
      "Epoch 32/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0889 - reconstruction_loss: 0.0894 - kl_loss: 0.1917 - val_total_loss: 10000.0967 - val_reconstruction_loss: 0.0952 - val_kl_loss: 0.1922\n",
      "Epoch 33/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0928 - reconstruction_loss: 0.0897 - kl_loss: 0.2375 - val_total_loss: 10000.0977 - val_reconstruction_loss: 0.0950 - val_kl_loss: 0.2741\n",
      "Epoch 34/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0938 - reconstruction_loss: 0.0915 - kl_loss: 0.2540 - val_total_loss: 10000.0977 - val_reconstruction_loss: 0.0960 - val_kl_loss: 0.2334\n",
      "Epoch 35/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0898 - reconstruction_loss: 0.0910 - kl_loss: 0.2325 - val_total_loss: 10000.0986 - val_reconstruction_loss: 0.0970 - val_kl_loss: 0.1516\n",
      "Epoch 36/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0957 - reconstruction_loss: 0.0920 - kl_loss: 0.1535 - val_total_loss: 10000.0986 - val_reconstruction_loss: 0.0960 - val_kl_loss: 0.1996\n",
      "Epoch 37/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0889 - reconstruction_loss: 0.0899 - kl_loss: 0.2119 - val_total_loss: 10000.0967 - val_reconstruction_loss: 0.0945 - val_kl_loss: 0.2420\n",
      "Epoch 38/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0879 - reconstruction_loss: 0.0886 - kl_loss: 0.2330 - val_total_loss: 10000.0967 - val_reconstruction_loss: 0.0935 - val_kl_loss: 0.2252\n",
      "Epoch 39/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0879 - reconstruction_loss: 0.0883 - kl_loss: 0.2505 - val_total_loss: 10000.0947 - val_reconstruction_loss: 0.0932 - val_kl_loss: 0.2624\n",
      "Epoch 40/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0889 - reconstruction_loss: 0.0904 - kl_loss: 0.2854 - val_total_loss: 10000.0957 - val_reconstruction_loss: 0.0941 - val_kl_loss: 0.2368\n",
      "Epoch 41/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0889 - reconstruction_loss: 0.0893 - kl_loss: 0.2892 - val_total_loss: 10000.0957 - val_reconstruction_loss: 0.0933 - val_kl_loss: 0.2804\n",
      "Epoch 42/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0879 - reconstruction_loss: 0.0881 - kl_loss: 0.2827 - val_total_loss: 10000.0957 - val_reconstruction_loss: 0.0923 - val_kl_loss: 0.3436\n",
      "Epoch 43/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0879 - reconstruction_loss: 0.0882 - kl_loss: 0.3803 - val_total_loss: 10000.0957 - val_reconstruction_loss: 0.0932 - val_kl_loss: 0.3105\n",
      "Epoch 44/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0918 - reconstruction_loss: 0.0904 - kl_loss: 0.3424 - val_total_loss: 10000.0957 - val_reconstruction_loss: 0.0930 - val_kl_loss: 0.3010\n",
      "Epoch 45/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0889 - reconstruction_loss: 0.0890 - kl_loss: 0.3392 - val_total_loss: 10000.0977 - val_reconstruction_loss: 0.0928 - val_kl_loss: 0.4247\n",
      "Epoch 46/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0879 - reconstruction_loss: 0.0883 - kl_loss: 0.3111 - val_total_loss: 10000.0947 - val_reconstruction_loss: 0.0925 - val_kl_loss: 0.2819\n",
      "Epoch 47/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0986 - reconstruction_loss: 0.0932 - kl_loss: 0.2719 - val_total_loss: 10000.1016 - val_reconstruction_loss: 0.1001 - val_kl_loss: 0.2535\n",
      "Epoch 48/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0986 - reconstruction_loss: 0.0933 - kl_loss: 0.2467 - val_total_loss: 10000.1025 - val_reconstruction_loss: 0.1000 - val_kl_loss: 0.2593\n",
      "Epoch 49/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0986 - reconstruction_loss: 0.0951 - kl_loss: 0.2196 - val_total_loss: 10000.0986 - val_reconstruction_loss: 0.0968 - val_kl_loss: 0.2174\n",
      "Epoch 50/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0957 - reconstruction_loss: 0.0917 - kl_loss: 0.2172 - val_total_loss: 10000.0986 - val_reconstruction_loss: 0.0966 - val_kl_loss: 0.2308\n",
      "Epoch 51/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0898 - reconstruction_loss: 0.0908 - kl_loss: 0.2153 - val_total_loss: 10000.0967 - val_reconstruction_loss: 0.0946 - val_kl_loss: 0.2150\n",
      "Epoch 52/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0889 - reconstruction_loss: 0.0895 - kl_loss: 0.2693 - val_total_loss: 10000.0967 - val_reconstruction_loss: 0.0939 - val_kl_loss: 0.2856\n",
      "Epoch 53/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0889 - reconstruction_loss: 0.0894 - kl_loss: 0.2463 - val_total_loss: 10000.0967 - val_reconstruction_loss: 0.0943 - val_kl_loss: 0.2757\n",
      "Epoch 54/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0889 - reconstruction_loss: 0.0887 - kl_loss: 0.2361 - val_total_loss: 10000.0957 - val_reconstruction_loss: 0.0937 - val_kl_loss: 0.3060\n",
      "Epoch 55/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0879 - reconstruction_loss: 0.0892 - kl_loss: 0.2985 - val_total_loss: 10000.0957 - val_reconstruction_loss: 0.0931 - val_kl_loss: 0.2862\n",
      "Epoch 56/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0908 - reconstruction_loss: 0.0885 - kl_loss: 0.2396 - val_total_loss: 10000.0947 - val_reconstruction_loss: 0.0932 - val_kl_loss: 0.1986\n",
      "Epoch 57/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0889 - reconstruction_loss: 0.0888 - kl_loss: 0.2376 - val_total_loss: 10000.0957 - val_reconstruction_loss: 0.0933 - val_kl_loss: 0.2778\n",
      "Epoch 58/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0879 - reconstruction_loss: 0.0874 - kl_loss: 0.2779 - val_total_loss: 10000.0947 - val_reconstruction_loss: 0.0921 - val_kl_loss: 0.3006\n",
      "Epoch 59/200\n",
      "64/64 [==============================] - 1s 9ms/step - total_loss: 10000.0869 - reconstruction_loss: 0.0864 - kl_loss: 0.2691 - val_total_loss: 10000.0947 - val_reconstruction_loss: 0.0920 - val_kl_loss: 0.2819\n",
      "Epoch 60/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0869 - reconstruction_loss: 0.0869 - kl_loss: 0.2780 - val_total_loss: 10000.0967 - val_reconstruction_loss: 0.0937 - val_kl_loss: 0.2079\n",
      "Epoch 61/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0879 - reconstruction_loss: 0.0887 - kl_loss: 0.1948 - val_total_loss: 10000.0947 - val_reconstruction_loss: 0.0924 - val_kl_loss: 0.2324\n",
      "Epoch 62/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0869 - reconstruction_loss: 0.0862 - kl_loss: 0.2414 - val_total_loss: 10000.0938 - val_reconstruction_loss: 0.0912 - val_kl_loss: 0.2678\n",
      "Epoch 63/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0869 - reconstruction_loss: 0.0856 - kl_loss: 0.2486 - val_total_loss: 10000.0938 - val_reconstruction_loss: 0.0913 - val_kl_loss: 0.2599\n",
      "Epoch 64/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0869 - reconstruction_loss: 0.0870 - kl_loss: 0.2773 - val_total_loss: 10000.0977 - val_reconstruction_loss: 0.0939 - val_kl_loss: 0.2849\n",
      "Epoch 65/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0869 - reconstruction_loss: 0.0872 - kl_loss: 0.2856 - val_total_loss: 10000.0947 - val_reconstruction_loss: 0.0920 - val_kl_loss: 0.3051\n",
      "Epoch 66/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0869 - reconstruction_loss: 0.0857 - kl_loss: 0.2987 - val_total_loss: 10000.0938 - val_reconstruction_loss: 0.0911 - val_kl_loss: 0.3237\n",
      "Epoch 67/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0869 - reconstruction_loss: 0.0853 - kl_loss: 0.2953 - val_total_loss: 10000.0938 - val_reconstruction_loss: 0.0913 - val_kl_loss: 0.2917\n",
      "Epoch 68/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0859 - reconstruction_loss: 0.0849 - kl_loss: 0.2726 - val_total_loss: 10000.0938 - val_reconstruction_loss: 0.0915 - val_kl_loss: 0.2635\n",
      "Epoch 69/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0859 - reconstruction_loss: 0.0846 - kl_loss: 0.3177 - val_total_loss: 10000.0938 - val_reconstruction_loss: 0.0901 - val_kl_loss: 0.3640\n",
      "Epoch 70/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0859 - reconstruction_loss: 0.0842 - kl_loss: 0.3340 - val_total_loss: 10000.0918 - val_reconstruction_loss: 0.0891 - val_kl_loss: 0.3268\n",
      "Epoch 71/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0859 - reconstruction_loss: 0.0843 - kl_loss: 0.3115 - val_total_loss: 10000.0918 - val_reconstruction_loss: 0.0898 - val_kl_loss: 0.3296\n",
      "Epoch 72/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0859 - reconstruction_loss: 0.0842 - kl_loss: 0.3267 - val_total_loss: 10000.0918 - val_reconstruction_loss: 0.0898 - val_kl_loss: 0.3160\n",
      "Epoch 73/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0869 - reconstruction_loss: 0.0830 - kl_loss: 0.4008 - val_total_loss: 10000.0918 - val_reconstruction_loss: 0.0890 - val_kl_loss: 0.3807\n",
      "Epoch 74/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0859 - reconstruction_loss: 0.0829 - kl_loss: 0.3496 - val_total_loss: 10000.0918 - val_reconstruction_loss: 0.0885 - val_kl_loss: 0.3556\n",
      "Epoch 75/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0850 - reconstruction_loss: 0.0823 - kl_loss: 0.3505 - val_total_loss: 10000.0908 - val_reconstruction_loss: 0.0878 - val_kl_loss: 0.3836\n",
      "Epoch 76/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0840 - reconstruction_loss: 0.0825 - kl_loss: 0.3635 - val_total_loss: 10000.0908 - val_reconstruction_loss: 0.0875 - val_kl_loss: 0.3864\n",
      "Epoch 77/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0859 - reconstruction_loss: 0.0822 - kl_loss: 0.3822 - val_total_loss: 10000.0918 - val_reconstruction_loss: 0.0876 - val_kl_loss: 0.4021\n",
      "Epoch 78/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0850 - reconstruction_loss: 0.0818 - kl_loss: 0.3807 - val_total_loss: 10000.0908 - val_reconstruction_loss: 0.0874 - val_kl_loss: 0.4012\n",
      "Epoch 79/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0840 - reconstruction_loss: 0.0819 - kl_loss: 0.3710 - val_total_loss: 10000.0918 - val_reconstruction_loss: 0.0873 - val_kl_loss: 0.4401\n",
      "Epoch 80/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0850 - reconstruction_loss: 0.0818 - kl_loss: 0.4029 - val_total_loss: 10000.0908 - val_reconstruction_loss: 0.0870 - val_kl_loss: 0.3735\n",
      "Epoch 81/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0840 - reconstruction_loss: 0.0817 - kl_loss: 0.3761 - val_total_loss: 10000.0908 - val_reconstruction_loss: 0.0870 - val_kl_loss: 0.3878\n",
      "Epoch 82/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0859 - reconstruction_loss: 0.0821 - kl_loss: 0.3867 - val_total_loss: 10000.0928 - val_reconstruction_loss: 0.0891 - val_kl_loss: 0.3991\n",
      "Epoch 83/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0859 - reconstruction_loss: 0.0826 - kl_loss: 0.4075 - val_total_loss: 10000.0918 - val_reconstruction_loss: 0.0878 - val_kl_loss: 0.4122\n",
      "Epoch 84/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0850 - reconstruction_loss: 0.0812 - kl_loss: 0.4069 - val_total_loss: 10000.0908 - val_reconstruction_loss: 0.0862 - val_kl_loss: 0.4220\n",
      "Epoch 85/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0957 - reconstruction_loss: 0.0863 - kl_loss: 0.3727 - val_total_loss: 10000.1006 - val_reconstruction_loss: 0.0957 - val_kl_loss: 0.4648\n",
      "Epoch 86/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0879 - reconstruction_loss: 0.0875 - kl_loss: 0.3584 - val_total_loss: 10000.0947 - val_reconstruction_loss: 0.0905 - val_kl_loss: 0.3725\n",
      "Epoch 87/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0869 - reconstruction_loss: 0.0841 - kl_loss: 0.3727 - val_total_loss: 10000.0928 - val_reconstruction_loss: 0.0882 - val_kl_loss: 0.4180\n",
      "Epoch 88/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0859 - reconstruction_loss: 0.0834 - kl_loss: 0.3728 - val_total_loss: 10000.0918 - val_reconstruction_loss: 0.0877 - val_kl_loss: 0.3891\n",
      "Epoch 89/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0859 - reconstruction_loss: 0.0826 - kl_loss: 0.3853 - val_total_loss: 10000.0908 - val_reconstruction_loss: 0.0865 - val_kl_loss: 0.4249\n",
      "Epoch 90/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0850 - reconstruction_loss: 0.0818 - kl_loss: 0.4036 - val_total_loss: 10000.0908 - val_reconstruction_loss: 0.0866 - val_kl_loss: 0.4164\n",
      "Epoch 91/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0859 - reconstruction_loss: 0.0811 - kl_loss: 0.4364 - val_total_loss: 10000.0908 - val_reconstruction_loss: 0.0854 - val_kl_loss: 0.4435\n",
      "Epoch 92/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0850 - reconstruction_loss: 0.0813 - kl_loss: 0.4276 - val_total_loss: 10000.0918 - val_reconstruction_loss: 0.0863 - val_kl_loss: 0.5210\n",
      "Epoch 93/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0850 - reconstruction_loss: 0.0811 - kl_loss: 0.4544 - val_total_loss: 10000.0908 - val_reconstruction_loss: 0.0859 - val_kl_loss: 0.4722\n",
      "Epoch 94/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0840 - reconstruction_loss: 0.0805 - kl_loss: 0.4186 - val_total_loss: 10000.0898 - val_reconstruction_loss: 0.0848 - val_kl_loss: 0.4518\n",
      "Epoch 95/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0840 - reconstruction_loss: 0.0816 - kl_loss: 0.4483 - val_total_loss: 10000.0928 - val_reconstruction_loss: 0.0883 - val_kl_loss: 0.4648\n",
      "Epoch 96/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0859 - reconstruction_loss: 0.0825 - kl_loss: 0.4564 - val_total_loss: 10000.0908 - val_reconstruction_loss: 0.0864 - val_kl_loss: 0.4690\n",
      "Epoch 97/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0840 - reconstruction_loss: 0.0807 - kl_loss: 0.4274 - val_total_loss: 10000.0908 - val_reconstruction_loss: 0.0854 - val_kl_loss: 0.4641\n",
      "Epoch 98/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0840 - reconstruction_loss: 0.0803 - kl_loss: 0.4456 - val_total_loss: 10000.0898 - val_reconstruction_loss: 0.0855 - val_kl_loss: 0.4268\n",
      "Epoch 99/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0798 - kl_loss: 0.4571 - val_total_loss: 10000.0898 - val_reconstruction_loss: 0.0842 - val_kl_loss: 0.5150\n",
      "Epoch 100/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0840 - reconstruction_loss: 0.0796 - kl_loss: 0.4616 - val_total_loss: 10000.0898 - val_reconstruction_loss: 0.0844 - val_kl_loss: 0.4500\n",
      "Epoch 101/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0792 - kl_loss: 0.4377 - val_total_loss: 10000.0889 - val_reconstruction_loss: 0.0843 - val_kl_loss: 0.4879\n",
      "Epoch 102/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0788 - kl_loss: 0.4675 - val_total_loss: 10000.0879 - val_reconstruction_loss: 0.0839 - val_kl_loss: 0.4796\n",
      "Epoch 103/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0791 - kl_loss: 0.4531 - val_total_loss: 10000.0898 - val_reconstruction_loss: 0.0844 - val_kl_loss: 0.4550\n",
      "Epoch 104/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0785 - kl_loss: 0.4470 - val_total_loss: 10000.0898 - val_reconstruction_loss: 0.0844 - val_kl_loss: 0.4588\n",
      "Epoch 105/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0786 - kl_loss: 0.4627 - val_total_loss: 10000.0889 - val_reconstruction_loss: 0.0839 - val_kl_loss: 0.4596\n",
      "Epoch 106/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0850 - reconstruction_loss: 0.0812 - kl_loss: 0.5379 - val_total_loss: 10000.0908 - val_reconstruction_loss: 0.0854 - val_kl_loss: 0.5210\n",
      "Epoch 107/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0840 - reconstruction_loss: 0.0797 - kl_loss: 0.4809 - val_total_loss: 10000.0898 - val_reconstruction_loss: 0.0837 - val_kl_loss: 0.5073\n",
      "Epoch 108/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0784 - kl_loss: 0.5172 - val_total_loss: 10000.0879 - val_reconstruction_loss: 0.0832 - val_kl_loss: 0.5316\n",
      "Epoch 109/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0796 - kl_loss: 0.5551 - val_total_loss: 10000.0918 - val_reconstruction_loss: 0.0863 - val_kl_loss: 0.5194\n",
      "Epoch 110/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0850 - reconstruction_loss: 0.0803 - kl_loss: 0.4979 - val_total_loss: 10000.0908 - val_reconstruction_loss: 0.0851 - val_kl_loss: 0.5127\n",
      "Epoch 111/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0789 - kl_loss: 0.4954 - val_total_loss: 10000.0889 - val_reconstruction_loss: 0.0836 - val_kl_loss: 0.5029\n",
      "Epoch 112/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0790 - kl_loss: 0.4713 - val_total_loss: 10000.0898 - val_reconstruction_loss: 0.0849 - val_kl_loss: 0.4598\n",
      "Epoch 113/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0789 - kl_loss: 0.4795 - val_total_loss: 10000.0879 - val_reconstruction_loss: 0.0834 - val_kl_loss: 0.4944\n",
      "Epoch 114/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0840 - reconstruction_loss: 0.0781 - kl_loss: 0.6627 - val_total_loss: 10000.0889 - val_reconstruction_loss: 0.0815 - val_kl_loss: 0.7385\n",
      "Epoch 115/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0782 - kl_loss: 0.5790 - val_total_loss: 10000.0879 - val_reconstruction_loss: 0.0832 - val_kl_loss: 0.5364\n",
      "Epoch 116/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0820 - reconstruction_loss: 0.0780 - kl_loss: 0.5140 - val_total_loss: 10000.0879 - val_reconstruction_loss: 0.0826 - val_kl_loss: 0.5144\n",
      "Epoch 117/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0776 - kl_loss: 0.5191 - val_total_loss: 10000.0889 - val_reconstruction_loss: 0.0824 - val_kl_loss: 0.6204\n",
      "Epoch 118/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0781 - kl_loss: 0.5245 - val_total_loss: 10000.0889 - val_reconstruction_loss: 0.0828 - val_kl_loss: 0.5192\n",
      "Epoch 119/200\n",
      "64/64 [==============================] - 1s 9ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0777 - kl_loss: 0.5166 - val_total_loss: 10000.0869 - val_reconstruction_loss: 0.0823 - val_kl_loss: 0.5017\n",
      "Epoch 120/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0779 - kl_loss: 0.6041 - val_total_loss: 10000.0879 - val_reconstruction_loss: 0.0826 - val_kl_loss: 0.5280\n",
      "Epoch 121/200\n",
      "64/64 [==============================] - 1s 9ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0771 - kl_loss: 0.5563 - val_total_loss: 10000.0898 - val_reconstruction_loss: 0.0827 - val_kl_loss: 0.6924\n",
      "Epoch 122/200\n",
      "64/64 [==============================] - 1s 9ms/step - total_loss: 10000.0869 - reconstruction_loss: 0.0831 - kl_loss: 0.7771 - val_total_loss: 10000.0918 - val_reconstruction_loss: 0.0893 - val_kl_loss: 0.3612\n",
      "Epoch 123/200\n",
      "64/64 [==============================] - 1s 9ms/step - total_loss: 10000.0850 - reconstruction_loss: 0.0811 - kl_loss: 0.4753 - val_total_loss: 10000.0898 - val_reconstruction_loss: 0.0859 - val_kl_loss: 0.4316\n",
      "Epoch 124/200\n",
      "64/64 [==============================] - 1s 9ms/step - total_loss: 10000.0879 - reconstruction_loss: 0.0803 - kl_loss: 0.5978 - val_total_loss: 10000.0947 - val_reconstruction_loss: 0.0881 - val_kl_loss: 0.7199\n",
      "Epoch 125/200\n",
      "64/64 [==============================] - 1s 9ms/step - total_loss: 10000.0869 - reconstruction_loss: 0.0815 - kl_loss: 0.4932 - val_total_loss: 10000.0898 - val_reconstruction_loss: 0.0859 - val_kl_loss: 0.4207\n",
      "Epoch 126/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0840 - reconstruction_loss: 0.0799 - kl_loss: 0.4446 - val_total_loss: 10000.0889 - val_reconstruction_loss: 0.0845 - val_kl_loss: 0.4487\n",
      "Epoch 127/200\n",
      "64/64 [==============================] - 1s 9ms/step - total_loss: 10000.0840 - reconstruction_loss: 0.0801 - kl_loss: 0.4410 - val_total_loss: 10000.0898 - val_reconstruction_loss: 0.0851 - val_kl_loss: 0.4489\n",
      "Epoch 128/200\n",
      "64/64 [==============================] - 1s 9ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0794 - kl_loss: 0.4597 - val_total_loss: 10000.0889 - val_reconstruction_loss: 0.0844 - val_kl_loss: 0.4499\n",
      "Epoch 129/200\n",
      "64/64 [==============================] - 1s 9ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0790 - kl_loss: 0.4553 - val_total_loss: 10000.0889 - val_reconstruction_loss: 0.0841 - val_kl_loss: 0.4439\n",
      "Epoch 130/200\n",
      "64/64 [==============================] - 1s 9ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0790 - kl_loss: 0.4681 - val_total_loss: 10000.0889 - val_reconstruction_loss: 0.0842 - val_kl_loss: 0.4926\n",
      "Epoch 131/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0789 - kl_loss: 0.5072 - val_total_loss: 10000.0908 - val_reconstruction_loss: 0.0869 - val_kl_loss: 0.4746\n",
      "Epoch 132/200\n",
      "64/64 [==============================] - 1s 9ms/step - total_loss: 10000.0840 - reconstruction_loss: 0.0797 - kl_loss: 0.5012 - val_total_loss: 10000.0889 - val_reconstruction_loss: 0.0842 - val_kl_loss: 0.4608\n",
      "Epoch 133/200\n",
      "64/64 [==============================] - 1s 9ms/step - total_loss: 10000.0840 - reconstruction_loss: 0.0784 - kl_loss: 0.4836 - val_total_loss: 10000.0879 - val_reconstruction_loss: 0.0831 - val_kl_loss: 0.5003\n",
      "Epoch 134/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0782 - kl_loss: 0.4982 - val_total_loss: 10000.0879 - val_reconstruction_loss: 0.0829 - val_kl_loss: 0.4707\n",
      "Epoch 135/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0778 - kl_loss: 0.4794 - val_total_loss: 10000.0869 - val_reconstruction_loss: 0.0824 - val_kl_loss: 0.5002\n",
      "Epoch 136/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0773 - kl_loss: 0.5286 - val_total_loss: 10000.0879 - val_reconstruction_loss: 0.0822 - val_kl_loss: 0.6121\n",
      "Epoch 137/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0777 - kl_loss: 0.5483 - val_total_loss: 10000.0889 - val_reconstruction_loss: 0.0839 - val_kl_loss: 0.4722\n",
      "Epoch 138/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0777 - kl_loss: 0.4873 - val_total_loss: 10000.0869 - val_reconstruction_loss: 0.0820 - val_kl_loss: 0.4856\n",
      "Epoch 139/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0820 - reconstruction_loss: 0.0768 - kl_loss: 0.5043 - val_total_loss: 10000.0859 - val_reconstruction_loss: 0.0814 - val_kl_loss: 0.5405\n",
      "Epoch 140/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0820 - reconstruction_loss: 0.0769 - kl_loss: 0.5140 - val_total_loss: 10000.0889 - val_reconstruction_loss: 0.0835 - val_kl_loss: 0.5677\n",
      "Epoch 141/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0778 - kl_loss: 0.5327 - val_total_loss: 10000.0879 - val_reconstruction_loss: 0.0832 - val_kl_loss: 0.5132\n",
      "Epoch 142/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0779 - kl_loss: 0.5091 - val_total_loss: 10000.0889 - val_reconstruction_loss: 0.0843 - val_kl_loss: 0.4947\n",
      "Epoch 143/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0859 - reconstruction_loss: 0.0787 - kl_loss: 0.6854 - val_total_loss: 10000.0889 - val_reconstruction_loss: 0.0829 - val_kl_loss: 0.5762\n",
      "Epoch 144/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0785 - kl_loss: 0.5200 - val_total_loss: 10000.0879 - val_reconstruction_loss: 0.0825 - val_kl_loss: 0.5254\n",
      "Epoch 145/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0788 - kl_loss: 0.4841 - val_total_loss: 10000.0879 - val_reconstruction_loss: 0.0836 - val_kl_loss: 0.4881\n",
      "Epoch 146/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0777 - kl_loss: 0.5002 - val_total_loss: 10000.0859 - val_reconstruction_loss: 0.0819 - val_kl_loss: 0.4954\n",
      "Epoch 147/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0769 - kl_loss: 0.5108 - val_total_loss: 10000.0869 - val_reconstruction_loss: 0.0819 - val_kl_loss: 0.5039\n",
      "Epoch 148/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0820 - reconstruction_loss: 0.0768 - kl_loss: 0.5036 - val_total_loss: 10000.0869 - val_reconstruction_loss: 0.0823 - val_kl_loss: 0.4880\n",
      "Epoch 149/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0820 - reconstruction_loss: 0.0767 - kl_loss: 0.4908 - val_total_loss: 10000.0869 - val_reconstruction_loss: 0.0825 - val_kl_loss: 0.4849\n",
      "Epoch 150/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0820 - reconstruction_loss: 0.0767 - kl_loss: 0.5379 - val_total_loss: 10000.0879 - val_reconstruction_loss: 0.0817 - val_kl_loss: 0.5696\n",
      "Epoch 151/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0792 - kl_loss: 0.6374 - val_total_loss: 10000.0947 - val_reconstruction_loss: 0.0871 - val_kl_loss: 0.7398\n",
      "Epoch 152/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0840 - reconstruction_loss: 0.0792 - kl_loss: 0.6755 - val_total_loss: 10000.0898 - val_reconstruction_loss: 0.0837 - val_kl_loss: 0.6692\n",
      "Epoch 153/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0787 - kl_loss: 0.5566 - val_total_loss: 10000.0879 - val_reconstruction_loss: 0.0833 - val_kl_loss: 0.4973\n",
      "Epoch 154/200\n",
      "64/64 [==============================] - 1s 11ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0777 - kl_loss: 0.5383 - val_total_loss: 10000.0879 - val_reconstruction_loss: 0.0820 - val_kl_loss: 0.5893\n",
      "Epoch 155/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0779 - kl_loss: 0.5530 - val_total_loss: 10000.0879 - val_reconstruction_loss: 0.0826 - val_kl_loss: 0.5624\n",
      "Epoch 156/200\n",
      "64/64 [==============================] - 1s 9ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0773 - kl_loss: 0.5607 - val_total_loss: 10000.0908 - val_reconstruction_loss: 0.0854 - val_kl_loss: 0.5220\n",
      "Epoch 157/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0778 - kl_loss: 0.5530 - val_total_loss: 10000.0869 - val_reconstruction_loss: 0.0819 - val_kl_loss: 0.5080\n",
      "Epoch 158/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0772 - kl_loss: 0.5786 - val_total_loss: 10000.0879 - val_reconstruction_loss: 0.0818 - val_kl_loss: 0.6363\n",
      "Epoch 159/200\n",
      "64/64 [==============================] - 1s 9ms/step - total_loss: 10000.0850 - reconstruction_loss: 0.0788 - kl_loss: 0.5788 - val_total_loss: 10000.0898 - val_reconstruction_loss: 0.0841 - val_kl_loss: 0.5017\n",
      "Epoch 160/200\n",
      "64/64 [==============================] - 1s 9ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0778 - kl_loss: 0.4930 - val_total_loss: 10000.0889 - val_reconstruction_loss: 0.0829 - val_kl_loss: 0.5162\n",
      "Epoch 161/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0769 - kl_loss: 0.5333 - val_total_loss: 10000.0879 - val_reconstruction_loss: 0.0826 - val_kl_loss: 0.5332\n",
      "Epoch 162/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0770 - kl_loss: 0.5564 - val_total_loss: 10000.0889 - val_reconstruction_loss: 0.0833 - val_kl_loss: 0.5193\n",
      "Epoch 163/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0768 - kl_loss: 0.5154 - val_total_loss: 10000.0879 - val_reconstruction_loss: 0.0822 - val_kl_loss: 0.5206\n",
      "Epoch 164/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0820 - reconstruction_loss: 0.0764 - kl_loss: 0.5187 - val_total_loss: 10000.0869 - val_reconstruction_loss: 0.0820 - val_kl_loss: 0.5011\n",
      "Epoch 165/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0820 - reconstruction_loss: 0.0760 - kl_loss: 0.5132 - val_total_loss: 10000.0869 - val_reconstruction_loss: 0.0817 - val_kl_loss: 0.5043\n",
      "Epoch 166/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0820 - reconstruction_loss: 0.0756 - kl_loss: 0.5463 - val_total_loss: 10000.0859 - val_reconstruction_loss: 0.0811 - val_kl_loss: 0.5390\n",
      "Epoch 167/200\n",
      "64/64 [==============================] - 1s 11ms/step - total_loss: 10000.0811 - reconstruction_loss: 0.0759 - kl_loss: 0.5033 - val_total_loss: 10000.0869 - val_reconstruction_loss: 0.0825 - val_kl_loss: 0.5016\n",
      "Epoch 168/200\n",
      "64/64 [==============================] - 1s 12ms/step - total_loss: 10000.0820 - reconstruction_loss: 0.0762 - kl_loss: 0.5093 - val_total_loss: 10000.0869 - val_reconstruction_loss: 0.0812 - val_kl_loss: 0.5536\n",
      "Epoch 169/200\n",
      "64/64 [==============================] - 1s 11ms/step - total_loss: 10000.0820 - reconstruction_loss: 0.0765 - kl_loss: 0.6339 - val_total_loss: 10000.0879 - val_reconstruction_loss: 0.0823 - val_kl_loss: 0.5745\n",
      "Epoch 170/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0765 - kl_loss: 0.5259 - val_total_loss: 10000.0869 - val_reconstruction_loss: 0.0814 - val_kl_loss: 0.5245\n",
      "Epoch 171/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0820 - reconstruction_loss: 0.0760 - kl_loss: 0.5367 - val_total_loss: 10000.0869 - val_reconstruction_loss: 0.0812 - val_kl_loss: 0.5427\n",
      "Epoch 172/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0820 - reconstruction_loss: 0.0760 - kl_loss: 0.5058 - val_total_loss: 10000.0859 - val_reconstruction_loss: 0.0811 - val_kl_loss: 0.5274\n",
      "Epoch 173/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0811 - reconstruction_loss: 0.0756 - kl_loss: 0.5232 - val_total_loss: 10000.0859 - val_reconstruction_loss: 0.0812 - val_kl_loss: 0.5164\n",
      "Epoch 174/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0811 - reconstruction_loss: 0.0752 - kl_loss: 0.5489 - val_total_loss: 10000.0850 - val_reconstruction_loss: 0.0801 - val_kl_loss: 0.5614\n",
      "Epoch 175/200\n",
      "64/64 [==============================] - 1s 11ms/step - total_loss: 10000.0820 - reconstruction_loss: 0.0757 - kl_loss: 0.5652 - val_total_loss: 10000.0869 - val_reconstruction_loss: 0.0814 - val_kl_loss: 0.5268\n",
      "Epoch 176/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0820 - reconstruction_loss: 0.0750 - kl_loss: 0.5663 - val_total_loss: 10000.0869 - val_reconstruction_loss: 0.0808 - val_kl_loss: 0.6088\n",
      "Epoch 177/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0820 - reconstruction_loss: 0.0749 - kl_loss: 0.5600 - val_total_loss: 10000.0859 - val_reconstruction_loss: 0.0810 - val_kl_loss: 0.5194\n",
      "Epoch 178/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0811 - reconstruction_loss: 0.0746 - kl_loss: 0.5681 - val_total_loss: 10000.0859 - val_reconstruction_loss: 0.0814 - val_kl_loss: 0.5459\n",
      "Epoch 179/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0811 - reconstruction_loss: 0.0754 - kl_loss: 0.5451 - val_total_loss: 10000.0908 - val_reconstruction_loss: 0.0854 - val_kl_loss: 0.5718\n",
      "Epoch 180/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0840 - reconstruction_loss: 0.0783 - kl_loss: 0.5599 - val_total_loss: 10000.0879 - val_reconstruction_loss: 0.0826 - val_kl_loss: 0.5127\n",
      "Epoch 181/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0766 - kl_loss: 0.5969 - val_total_loss: 10000.0879 - val_reconstruction_loss: 0.0827 - val_kl_loss: 0.5539\n",
      "Epoch 182/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0773 - kl_loss: 0.6033 - val_total_loss: 10000.0879 - val_reconstruction_loss: 0.0814 - val_kl_loss: 0.5841\n",
      "Epoch 183/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0820 - reconstruction_loss: 0.0764 - kl_loss: 0.5165 - val_total_loss: 10000.0879 - val_reconstruction_loss: 0.0825 - val_kl_loss: 0.4768\n",
      "Epoch 184/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0820 - reconstruction_loss: 0.0770 - kl_loss: 0.4821 - val_total_loss: 10000.0859 - val_reconstruction_loss: 0.0813 - val_kl_loss: 0.5062\n",
      "Epoch 185/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0801 - reconstruction_loss: 0.0753 - kl_loss: 0.5273 - val_total_loss: 10000.0859 - val_reconstruction_loss: 0.0805 - val_kl_loss: 0.5520\n",
      "Epoch 186/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0801 - reconstruction_loss: 0.0751 - kl_loss: 0.5424 - val_total_loss: 10000.0850 - val_reconstruction_loss: 0.0809 - val_kl_loss: 0.4985\n",
      "Epoch 187/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0820 - reconstruction_loss: 0.0755 - kl_loss: 0.4949 - val_total_loss: 10000.0859 - val_reconstruction_loss: 0.0813 - val_kl_loss: 0.5035\n",
      "Epoch 188/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0811 - reconstruction_loss: 0.0748 - kl_loss: 0.5374 - val_total_loss: 10000.0859 - val_reconstruction_loss: 0.0809 - val_kl_loss: 0.5017\n",
      "Epoch 189/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0801 - reconstruction_loss: 0.0750 - kl_loss: 0.5106 - val_total_loss: 10000.0859 - val_reconstruction_loss: 0.0807 - val_kl_loss: 0.5102\n",
      "Epoch 190/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0801 - reconstruction_loss: 0.0748 - kl_loss: 0.5276 - val_total_loss: 10000.0850 - val_reconstruction_loss: 0.0802 - val_kl_loss: 0.4970\n",
      "Epoch 191/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0850 - reconstruction_loss: 0.0764 - kl_loss: 0.7776 - val_total_loss: 10000.0889 - val_reconstruction_loss: 0.0826 - val_kl_loss: 0.6031\n",
      "Epoch 192/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0820 - reconstruction_loss: 0.0759 - kl_loss: 0.5739 - val_total_loss: 10000.0869 - val_reconstruction_loss: 0.0812 - val_kl_loss: 0.5722\n",
      "Epoch 193/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0811 - reconstruction_loss: 0.0750 - kl_loss: 0.5634 - val_total_loss: 10000.0869 - val_reconstruction_loss: 0.0809 - val_kl_loss: 0.5342\n",
      "Epoch 194/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0811 - reconstruction_loss: 0.0752 - kl_loss: 0.5475 - val_total_loss: 10000.0859 - val_reconstruction_loss: 0.0800 - val_kl_loss: 0.5849\n",
      "Epoch 195/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0811 - reconstruction_loss: 0.0750 - kl_loss: 0.5536 - val_total_loss: 10000.0859 - val_reconstruction_loss: 0.0805 - val_kl_loss: 0.5387\n",
      "Epoch 196/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0811 - reconstruction_loss: 0.0746 - kl_loss: 0.5509 - val_total_loss: 10000.0850 - val_reconstruction_loss: 0.0802 - val_kl_loss: 0.5355\n",
      "Epoch 197/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0850 - reconstruction_loss: 0.0776 - kl_loss: 0.5788 - val_total_loss: 10000.0967 - val_reconstruction_loss: 0.0878 - val_kl_loss: 0.9616\n",
      "Epoch 198/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0869 - reconstruction_loss: 0.0778 - kl_loss: 0.8074 - val_total_loss: 10000.0898 - val_reconstruction_loss: 0.0826 - val_kl_loss: 0.6475\n",
      "Epoch 199/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0830 - reconstruction_loss: 0.0763 - kl_loss: 0.5687 - val_total_loss: 10000.0869 - val_reconstruction_loss: 0.0815 - val_kl_loss: 0.5384\n",
      "Epoch 200/200\n",
      "64/64 [==============================] - 1s 10ms/step - total_loss: 10000.0820 - reconstruction_loss: 0.0757 - kl_loss: 0.5407 - val_total_loss: 10000.0869 - val_reconstruction_loss: 0.0805 - val_kl_loss: 0.5487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fe67f7d0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "vae.fit(data, epochs = EPOCHS, batch_size = BATCH_SIZE, shuffle = True, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_board(tensor):\n",
    "    board = [['.' for j in range(8)] for i in range(8)]\n",
    "    for i, p in enumerate('KQRBNPkqrbnp'):\n",
    "        for r in range(8):\n",
    "            for c in range(8):\n",
    "                if tensor[r][c][i] > 0.25:\n",
    "                    if board[r][c] != '.':\n",
    "                        print(f'conflict between {board[r][c]} and {p} at position ({r},{c})')\n",
    "                        continue\n",
    "                    board[r][c] = p\n",
    "    pprint.pprint(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['r', 'n', 'b', 'q', 'k', 'b', 'n', 'r'],\n",
      " ['p', 'p', 'p', 'p', 'p', 'p', 'p', 'p'],\n",
      " ['.', '.', '.', '.', 'p', 'n', '.', '.'],\n",
      " ['.', '.', '.', '.', '.', '.', '.', '.'],\n",
      " ['.', '.', 'P', 'P', '.', '.', '.', '.'],\n",
      " ['.', '.', 'N', '.', '.', 'N', '.', '.'],\n",
      " ['P', 'P', 'P', 'P', 'P', 'P', 'P', 'P'],\n",
      " ['R', 'N', 'B', 'Q', 'K', 'B', 'N', 'R']]\n",
      "\n",
      "[['r', '.', '.', '.', '.', 'r', 'k', '.'],\n",
      " ['p', 'p', '.', '.', '.', 'p', 'p', 'p'],\n",
      " ['.', '.', '.', '.', '.', '.', '.', '.'],\n",
      " ['.', '.', '.', '.', '.', '.', '.', '.'],\n",
      " ['.', '.', '.', '.', '.', '.', '.', '.'],\n",
      " ['.', '.', '.', '.', '.', '.', '.', '.'],\n",
      " ['P', 'P', '.', '.', '.', 'P', 'P', 'P'],\n",
      " ['R', '.', '.', '.', '.', 'R', 'K', '.']]\n",
      "\n",
      "[['r', '.', '.', '.', '.', 'r', 'k', '.'],\n",
      " ['p', 'p', '.', '.', '.', 'p', 'p', 'p'],\n",
      " ['.', '.', '.', '.', '.', '.', '.', '.'],\n",
      " ['.', '.', '.', '.', '.', '.', '.', '.'],\n",
      " ['.', '.', '.', '.', '.', '.', '.', '.'],\n",
      " ['.', '.', '.', '.', '.', '.', '.', '.'],\n",
      " ['P', 'P', '.', '.', '.', 'P', 'P', 'P'],\n",
      " ['.', '.', '.', '.', '.', '.', 'K', '.']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for reconstruction in vae.generate_sample(3):\n",
    "    print_board(reconstruction)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
